
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>References and further reading</TITLE>
<META NAME="description" CONTENT="References and further reading">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="previous" HREF="latent-semantic-indexing-1.html">
<LINK REL="up" HREF="matrix-decompositions-and-latent-semantic-indexing-1.html">
<LINK REL="next" HREF="web-search-basics-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html4595"
  HREF="web-search-basics-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html4589"
  HREF="matrix-decompositions-and-latent-semantic-indexing-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html4585"
  HREF="latent-semantic-indexing-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html4591"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html4593"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html4596"
  HREF="web-search-basics-1.html">Web search basics</A>
<B> Up:</B> <A NAME="tex2html4590"
  HREF="matrix-decompositions-and-latent-semantic-indexing-1.html">Matrix decompositions and latent</A>
<B> Previous:</B> <A NAME="tex2html4586"
  HREF="latent-semantic-indexing-1.html">Latent semantic indexing</A>
 &nbsp; <B>  <A NAME="tex2html4592"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html4594"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION002350000000000000000"></A>
<A NAME="sec:furtherlsi"></A> <A NAME="p:furtherlsi"></A>
<BR>
References and further reading
</H1> 

<P>
<A
 HREF="bibliography-1.html#strang">Strang (1986)</A> provides an excellent introductory overview
of matrix decompositions including the singular value
decomposition. Theorem&nbsp;<A HREF="low-rank-approximations-1.html#thm:eckartyoung">18.3</A> is due to
<A
 HREF="bibliography-1.html#eckartyoung">Eckart and Young (1936)</A>. The connection between information
retrieval and low-rank approximations of the term-document
matrix was introduced in <A
 HREF="bibliography-1.html#dee90">Deerwester et&nbsp;al. (1990)</A>, with a subsequent
survey of results in <A
 HREF="bibliography-1.html#berrydumais95">Berry et&nbsp;al. (1995)</A>. <A
 HREF="bibliography-1.html#dumais93">Dumais (1993)</A>
and <A
 HREF="bibliography-1.html#Dum-95">Dumais (1995)</A> describe experiments on TREC benchmarks
giving evidence that at least on some benchmarks, LSI can
produce better precision and recall than standard
vector-space
retrieval. <TT><A NAME="tex2html196"
  HREF="http://www.cs.utk.edu/<TT>~</TT>berry/lsi++/">http://www.cs.utk.edu/<TT>~</TT>berry/lsi++/</A></TT>and <TT><A NAME="tex2html197"
  HREF="http://lsi.argreenhouse.com/lsi/LSIpapers.html">http://lsi.argreenhouse.com/lsi/LSIpapers.html</A></TT>offer comprehensive pointers to the literature and software
of LSI. <A
 HREF="bibliography-1.html#schutze97projections">Sch&#252;tze and Silverstein (1997)</A> evaluate
LSI and truncated representations of centroids for efficient
 <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means clustering
(Section <A HREF="k-means-1.html#sec:kmeans">16.4</A> ).
<A
 HREF="bibliography-1.html#bast05spectral">Bast and Majumdar (2005)</A> detail the role of the reduced
dimension <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$k$"> in LSI and how different pairs of terms get
coalesced together at differing values of <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$k$">.  Applications
of LSI to <A NAME="29070"></A> <I>cross-language information retrieval</I> 
(where documents in two or more different languages are
indexed, and a query posed in one language is expected to
retrieve documents in other languages) are developed in
<A
 HREF="bibliography-1.html#berryyoung1995">Berry and Young (1995)</A> and <A
 HREF="bibliography-1.html#littman98automatic">Littman et&nbsp;al. (1998)</A>.  LSI
(referred to as LSA in more general settings) has been
applied to host of other problems in computer science
ranging from memory modeling to computer vision.

<P>
<A NAME="tex2html4597"
  HREF="bibliography-1.html#hofmann99probabilistic">Hofmann (1999a</A>;<A NAME="tex2html4598"
  HREF="bibliography-1.html#th:plsi">b)</A> provides an initial
probabilistic extension of the basic latent semantic indexing
technique. A more satisfactory formal basis for a probabilistic latent
variable model for dimensionality reduction is the <A NAME="29075"></A> <I>Latent
  Dirichlet Allocation</I>  (<A NAME="29077"></A> <I>LDA</I> ) model
(<A
 HREF="bibliography-1.html#blei03latent">Blei et&nbsp;al., 2003</A>), which is generative and assigns probabilities to
documents outside of the training set.  This model is extended to a
hierarchical clustering by <A
 HREF="bibliography-1.html#rosenzvi04authortopic">Rosen-Zvi et&nbsp;al. (2004)</A>.
<A
 HREF="bibliography-1.html#wei06lda">Wei and Croft (2006)</A> present the first large scale evaluation of LDA,
finding it to significantly outperform the query likelihood model of
Section&nbsp;<A HREF="the-query-likelihood-model-1.html#sec:qlm">12.2</A> (page&nbsp;<A HREF="the-query-likelihood-model-1.html#p:qlm"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/crossref.png"></A>), but to not perform quite as well as the relevance
model mentioned in Section&nbsp;<A HREF="extended-language-modeling-approaches-1.html#sec:extended-lm">12.4</A> (page&nbsp;<A HREF="extended-language-modeling-approaches-1.html#p:extended-lm"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/crossref.png"></A>) - but the latter does
additional per-query processing unlike LDA.  <A
 HREF="bibliography-1.html#teh06hdp">Teh et&nbsp;al. (2006)</A>
generalize further by presenting <A NAME="29087"></A> <I>Hierarchical Dirichlet
Processes</I> , a probabilistic model which allows a group (for us, a
document) to be drawn from an infinite mixture of latent topics, while
still allowing these topics to be shared across documents.

<P>
<B>Exercises.</B>
<UL>
<LI><A NAME="ex:engspan"></A>Assume you have a set of documents each of which is in either English or in Spanish. The collection is given in Figure <A HREF="#fig:engspandocs">18.4</A> .

<P>

<DIV ALIGN="CENTER"><A NAME="fig:engspandocs"></A><A NAME="p:engspandocs"></A><A NAME="29149"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure:</STRONG>
Documents for Exercise&nbsp;<A HREF="#ex:engspan">18.5</A>.</CAPTION>
<TR><TD><IMG
 WIDTH="205" HEIGHT="139" BORDER="0"
 SRC="img1823.png"
 ALT="\begin{figure}\begin{tabular}{\vert\vert l\vert l\vert\vert}
\hline
DocID &amp; Docu...
...bienvenido \\
\hline
6 &amp; hello and welcome\\
\hline
\end{tabular}
\end{figure}"></TD></TR>
</TABLE>
</DIV>

<P>
Figure&nbsp;<A HREF="#engspangloss">18.5</A> gives a glossary relating the Spanish and English words above for your own information. This glossary is NOT available to the retrieval system:

<P>

<DIV ALIGN="CENTER"><A NAME="engspangloss"></A><A NAME="29150"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 18.5:</STRONG>
Glossary for Exercise&nbsp;<A HREF="#ex:engspan">18.5</A>.</CAPTION>
<TR><TD><IMG
 WIDTH="171" HEIGHT="136" BORDER="0"
 SRC="img1824.png"
 ALT="\begin{figure}\begin{tabular}{\vert\vert l\vert l\vert\vert}
\hline
Spanish &amp; En...
...rofessor\\
y &amp; and\\
bienvenido &amp; welcome\\
\hline
\end{tabular}
\end{figure}"></TD></TR>
</TABLE>
</DIV>

<P>

<OL>
<LI>Construct the appropriate term-document matrix <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1665.png"
 ALT="$\lsimatrix$"> to use for a collection consisting of these documents. For simplicity, use raw term frequencies rather than normalized tf-idf weights. Make sure to clearly label the dimensions of your matrix.
</LI>
<LI>Write down the matrices <IMG
 WIDTH="47" HEIGHT="35" ALIGN="MIDDLE" BORDER="0"
 SRC="img1825.png"
 ALT="$U_2,\Sigma'_2$"> and <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1826.png"
 ALT="$V_2$"> and from these derive the rank 2 approximation <IMG
 WIDTH="21" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1819.png"
 ALT="$\lsimatrix_2$">.
</LI>
<LI>State succinctly what the <IMG
 WIDTH="34" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$(i,j)$"> entry in the matrix <!-- MATH
 $\lsimatrix^T\lsimatrix$
 -->
<IMG
 WIDTH="35" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img1721.png"
 ALT="$\lsimatrix^T\lsimatrix$"> represents.
</LI>
<LI>State succinctly what the <IMG
 WIDTH="34" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.png"
 ALT="$(i,j)$"> entry in the matrix <!-- MATH
 $\lsimatrix_2^T\lsimatrix_2$
 -->
<IMG
 WIDTH="41" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img1827.png"
 ALT="$\lsimatrix_2^T\lsimatrix_2$"> represents, and why it differs from that in <!-- MATH
 $\lsimatrix^T\lsimatrix$
 -->
<IMG
 WIDTH="35" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img1721.png"
 ALT="$\lsimatrix^T\lsimatrix$">.
</LI>
</OL>

<P>
</LI>
</UL>

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html4595"
  HREF="web-search-basics-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html4589"
  HREF="matrix-decompositions-and-latent-semantic-indexing-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html4585"
  HREF="latent-semantic-indexing-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html4591"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html4593"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html4596"
  HREF="web-search-basics-1.html">Web search basics</A>
<B> Up:</B> <A NAME="tex2html4590"
  HREF="matrix-decompositions-and-latent-semantic-indexing-1.html">Matrix decompositions and latent</A>
<B> Previous:</B> <A NAME="tex2html4586"
  HREF="latent-semantic-indexing-1.html">Latent semantic indexing</A>
 &nbsp; <B>  <A NAME="tex2html4592"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html4594"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
