
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>References and further reading</TITLE>
<META NAME="description" CONTENT="References and further reading">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="exercises-3.html">
<LINK REL="previous" HREF="model-based-clustering-1.html">
<LINK REL="up" HREF="flat-clustering-1.html">
<LINK REL="next" HREF="exercises-3.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html4266"
  HREF="exercises-3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html4260"
  HREF="flat-clustering-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html4254"
  HREF="model-based-clustering-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html4262"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html4264"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html4267"
  HREF="exercises-3.html">Exercises</A>
<B> Up:</B> <A NAME="tex2html4261"
  HREF="flat-clustering-1.html">Flat clustering</A>
<B> Previous:</B> <A NAME="tex2html4255"
  HREF="model-based-clustering-1.html">Model-based clustering</A>
 &nbsp; <B>  <A NAME="tex2html4263"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html4265"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION002160000000000000000"></A>
<A NAME="sec:flatclustref"></A> <A NAME="p:flatclustref"></A>
<BR>
References and further reading
</H1> 

<P>
<A
 HREF="bibliography-1.html#berkhin06survey">Berkhin (2006b)</A> gives a general up-to-date survey of
clustering methods with special attention to scalability.
The classic reference for clustering in pattern recognition,
covering both  <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means and EM, is
(<A
 HREF="bibliography-1.html#duda00pattern">Duda et&nbsp;al., 2000</A>).  <A
 HREF="bibliography-1.html#rasmussen92">Rasmussen (1992)</A> introduces
clustering from an information retrieval perspective.
<A
 HREF="bibliography-1.html#anderberg73cluster">Anderberg (1973)</A> provides a general introduction to
clustering for applications. In addition to <A NAME="25072"></A> <I>Euclidean
distance</I>  and <A NAME="25074"></A> <I>cosine similarity</I> ,
<A NAME="25076"></A> <I>Kullback-Leibler divergence</I>  is often used in
clustering as a
measure of how (dis)similar documents and clusters are
(<A NAME="tex2html4268"
  HREF="bibliography-1.html#xu99clusterbased">Xu and Croft, 1999</A>, <A NAME="tex2html4269"
  HREF="bibliography-1.html#muresan04topic">Muresan and Harper, 2004</A>, <A NAME="tex2html4270"
  HREF="bibliography-1.html#kurland04corpus">Kurland and Lee, 2004</A>).

<P>
The cluster hypothesis is due to <A
 HREF="bibliography-1.html#jardine71hierarchic">Jardine and van&nbsp;Rijsbergen (1971)</A>
who state it as follows: Associations between
documents convey information about the relevance of
documents to requests.  
<A NAME="tex2html4271"
  HREF="bibliography-1.html#croft78cluster">Croft (1978)</A>, <A NAME="tex2html4272"
  HREF="bibliography-1.html#can90concepts">Can and Ozkarahan (1990)</A>, <A NAME="tex2html4273"
  HREF="bibliography-1.html#voorhees85">Voorhees (1985a)</A>, <A NAME="tex2html4274"
  HREF="bibliography-1.html#salton75dynamic">Salton (1975)</A>, <A NAME="tex2html4275"
  HREF="bibliography-1.html#cacheda03optimization">Cacheda et&nbsp;al. (2003)</A>, <A NAME="tex2html4276"
  HREF="bibliography-1.html#salton71cluster">Salton (1971a)</A>, <A NAME="tex2html4277"
  HREF="bibliography-1.html#singitham04efficiency">Singitham et&nbsp;al. (2004)</A>, <A NAME="tex2html4278"
  HREF="bibliography-1.html#can04efficiency">Can et&nbsp;al. (2004)</A>
and <A
 HREF="bibliography-1.html#altingovde08incremental">Alting&#246;vde et&nbsp;al. (2008)</A>
investigate the efficiency and effectiveness of
cluster-based retrieval. While some of these studies show
improvements in effectiveness, efficiency or both, there is
no consensus that cluster-based retrieval works well
consistently across scenarios.
Cluster-based language
modeling was pioneered by <A
 HREF="bibliography-1.html#liu04cluster">Liu and Croft (2004)</A>.

<P>
There is good evidence that clustering of search results
improves user experience and search result quality
(<A NAME="tex2html4279"
  HREF="bibliography-1.html#hp96">Hearst and Pedersen, 1996</A>, <A NAME="tex2html4280"
  HREF="bibliography-1.html#zamir99grouper">Zamir and Etzioni, 1999</A>, <A NAME="tex2html4281"
  HREF="bibliography-1.html#kaki05findex">K&#228;ki, 2005</A>, <A NAME="tex2html4282"
  HREF="bibliography-1.html#toda05search">Toda and Kataoka, 2005</A>, <A NAME="tex2html4283"
  HREF="bibliography-1.html#tombros02effectiveness">Tombros et&nbsp;al., 2002</A>), although not as much as search result
structuring based on carefully edited category hierarchies
(<A
 HREF="bibliography-1.html#hearst06clustering">Hearst, 2006</A>).  The Scatter-Gather interface
for browsing collections was presented by
<A
 HREF="bibliography-1.html#cutting92scattergather">Cutting et&nbsp;al. (1992)</A>.  A theoretical framework for
analyzing the properties of Scatter/Gather and other
information seeking user interfaces is presented by
<A
 HREF="bibliography-1.html#pirolli07information">Pirolli (2007)</A>.  <A
 HREF="bibliography-1.html#schutze97projections">Sch&#252;tze and Silverstein (1997)</A> evaluate LSI (Chapter <A HREF="matrix-decompositions-and-latent-semantic-indexing-1.html#ch:lsi">18</A> ) and
truncated representations of centroids for efficient  <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means
clustering.

<P>
The Columbia NewsBlaster system (<A
 HREF="bibliography-1.html#mckeown02news">McKeown et&nbsp;al., 2002</A>), a
forerunner to the now much more famous and refined Google
News (<TT><A NAME="tex2html186"
  HREF="http://news.google.com">http://news.google.com</A></TT>), used hierarchical
clustering (Chapter <A HREF="hierarchical-clustering-1.html#ch:hierclust">17</A> ) to give two levels of news
topic granularity.  See <A
 HREF="bibliography-1.html#hatzivassiloglou00linguistic">Hatzivassiloglou et&nbsp;al. (2000)</A>
for details, and
<A
 HREF="bibliography-1.html#chen00multilingual">Chen and Lin (2000)</A> and <A
 HREF="bibliography-1.html#radev01interactive">Radev et&nbsp;al. (2001)</A> for related
systems.  Other applications of clustering in information
retrieval are duplicate detection (<A
 HREF="bibliography-1.html#yang06near">Yang and Callan (2006)</A>,
shingling), novelty detection (see references
in hclstfurther) and <A NAME="25100"></A> <I>metadata</I> 
discovery on the semantic web (<A
 HREF="bibliography-1.html#alonso06gio">Alonso et&nbsp;al., 2006</A>).

<P>
The discussion of external evaluation measures is partially
based on <A
 HREF="bibliography-1.html#strehl02relationship">Strehl (2002)</A>.
<A
 HREF="bibliography-1.html#dom02information">Dom (2002)</A> proposes a measure <IMG
 WIDTH="24" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1535.png"
 ALT="$Q_0$"> that is
better motivated theoretically than NMI. <IMG
 WIDTH="24" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1535.png"
 ALT="$Q_0$"> is the number
of bits needed to transmit class memberships assuming
cluster memberships are known.  The Rand index is due to
<A
 HREF="bibliography-1.html#rand71objective">Rand (1971)</A>.  <A
 HREF="bibliography-1.html#hubert85comparing">Hubert and Arabie (1985)</A> propose an
<A NAME="25107"></A> <I>adjusted</I>  that ranges between <IMG
 WIDTH="25" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1267.png"
 ALT="$-1$"> and 1 and is 0 if there
is only chance agreement between clusters and classes
(similar to <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img754.png"
 ALT="$\kappa$"><A NAME="25109"></A> in Chapter <A HREF="evaluation-in-information-retrieval-1.html#ch:evaluation">8</A> ,
page <A HREF="assessing-relevance-1.html#p:kappa">8.2</A> ).  <A
 HREF="bibliography-1.html#basu04active">Basu et&nbsp;al. (2004)</A> argue that the
three evaluation measures NMI, Rand index and F measure give
very similar results.  <A
 HREF="bibliography-1.html#stein03cluster">Stein et&nbsp;al. (2003)</A> propose
<A NAME="25114"></A> <I>expected edge density</I>  as
an internal measure and give evidence that it is a good
predictor of the quality of a clustering.

<P>
<A
 HREF="bibliography-1.html#kleinberg02impossibility">Kleinberg (2002)</A> and
<A
 HREF="bibliography-1.html#meila05clusterings">Meila (2005)</A> present axiomatic frameworks for
comparing clusterings.

<P>
Authors that are often credited with the invention of the
 <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means algorithm include <A
 HREF="bibliography-1.html#lloyd82least">Lloyd (1982)</A> (first
distributed in 1957), <A
 HREF="bibliography-1.html#ball65data">Ball (1965)</A>,
<A
 HREF="bibliography-1.html#macqueen67some">MacQueen (1967)</A>, and <A
 HREF="bibliography-1.html#hartigan79kmeans">Hartigan and Wong (1979)</A>.
<A
 HREF="bibliography-1.html#arthur06worstcase">Arthur and Vassilvitskii (2006)</A> investigate the worst-case
complexity of  <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means. 
<A NAME="tex2html4284"
  HREF="bibliography-1.html#bradley98refining">Bradley and Fayyad (1998)</A>, <A NAME="tex2html4285"
  HREF="bibliography-1.html#pelleg99accelerating">Pelleg and Moore (1999)</A> and
<A
 HREF="bibliography-1.html#davidson03speeding">Davidson and Satyanarayana (2003)</A> investigate the convergence
properties 
of  <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means empirically and how it depends on
initial seed selection.
<A
 HREF="bibliography-1.html#dhillon01concept">Dhillon and Modha (2001)</A> compare
 <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means clusters with <A NAME="25130"></A> <I>SVD</I> -based clusters (Chapter <A HREF="matrix-decompositions-and-latent-semantic-indexing-1.html#ch:lsi">18</A> ).  The
K-medoid algorithm was presented by <A
 HREF="bibliography-1.html#kaufman90finding">Kaufman and Rousseeuw (1990)</A>.
The EM algorithm was originally introduced by <A
 HREF="bibliography-1.html#dlr77">Dempster et&nbsp;al. (1977)</A>.
An in-depth treatment of EM is (<A
 HREF="bibliography-1.html#mclachlan96em">McLachlan and Krishnan, 1996</A>). See
Section&nbsp;<A HREF="references-and-further-reading-18.html#sec:furtherlsi">18.5</A> (page&nbsp;<A HREF="references-and-further-reading-18.html#p:furtherlsi"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/crossref.png"></A>) for publications on latent analysis, which can
also be viewed as soft clustering.

<P>
AIC is due to <A
 HREF="bibliography-1.html#akaike74new">Akaike (1974)</A> (see also
<A
 HREF="bibliography-1.html#burnham02model">Burnham and Anderson (2002)</A>). An alternative to AIC is BIC, which
can be motivated as a Bayesian model selection procedure
(<A
 HREF="bibliography-1.html#schwarz78estimating">Schwarz, 1978</A>).  <A
 HREF="bibliography-1.html#fraley98how">Fraley and Raftery (1998)</A> show how to
choose an optimal number of clusters based on BIC. An
application of BIC to  <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$">-means is (<A
 HREF="bibliography-1.html#pelleg00xmeans">Pelleg and Moore, 2000</A>).
<A
 HREF="bibliography-1.html#hamerly03kmeans">Hamerly and Elkan (2003)</A> propose an alternative to BIC that
performs better in their experiments.  Another influential
Bayesian approach for determining the number of clusters
(simultaneously with cluster assignment) is described by
<A
 HREF="bibliography-1.html#cheeseman96bayesian">Cheeseman and Stutz (1996)</A>.  Two methods for determining
cardinality without external criteria are presented by
<A
 HREF="bibliography-1.html#tibshirani01estimating">Tibshirani et&nbsp;al. (2001)</A>.

<P>
We only have space here for classical completely
unsupervised clustering. An important current topic of
research is how to use prior knowledge to guide clustering
(e.g., <A
 HREF="bibliography-1.html#ji06document">Ji and Xu (2006)</A>) and how to incorporate
interactive feedback during clustering (e.g.,
<A
 HREF="bibliography-1.html#huang06text">Huang and Mitchell (2006)</A>).  <A
 HREF="bibliography-1.html#fayyad98initialization">Fayyad et&nbsp;al. (1998)</A> propose
an initialization for EM clustering.  For algorithms that
can cluster very large data sets in one scan through the
data see <A
 HREF="bibliography-1.html#bradley98scaling">Bradley et&nbsp;al. (1998)</A>.

<P>
The applications in Table <A HREF="clustering-in-information-retrieval-1.html#tab:clusttb1">16.1</A>  all cluster
documents. Other information retrieval applications cluster
words (e.g., <A
 HREF="bibliography-1.html#crouch88cluster">Crouch, 1988</A>),  contexts of words
(e.g., <A
 HREF="bibliography-1.html#schuetze95information">Sch&#252;tze and Pedersen, 1995</A>) or words and
documents simultaneously
(e.g., <A NAME="tex2html4286"
  HREF="bibliography-1.html#tishby00data">Tishby and Slonim, 2000</A>, <A NAME="tex2html4287"
  HREF="bibliography-1.html#zha01bipartite">Zha et&nbsp;al., 2001</A>, <A NAME="tex2html4288"
  HREF="bibliography-1.html#dhillon01coclustering">Dhillon, 2001</A>).
Simultaneous clustering of words and documents is an example
of
<A NAME="25155"></A> <I>co-clustering</I>  or <A NAME="25157"></A> <I>biclustering</I> .

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html4266"
  HREF="exercises-3.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html4260"
  HREF="flat-clustering-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html4254"
  HREF="model-based-clustering-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html4262"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html4264"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html4267"
  HREF="exercises-3.html">Exercises</A>
<B> Up:</B> <A NAME="tex2html4261"
  HREF="flat-clustering-1.html">Flat clustering</A>
<B> Previous:</B> <A NAME="tex2html4255"
  HREF="model-based-clustering-1.html">Model-based clustering</A>
 &nbsp; <B>  <A NAME="tex2html4263"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html4265"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
