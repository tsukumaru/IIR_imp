
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Crawler architecture</TITLE>
<META NAME="description" CONTENT="Crawler architecture">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="dns-resolution-1.html">
<LINK REL="previous" HREF="crawling-1.html">
<LINK REL="up" HREF="crawling-1.html">
<LINK REL="next" HREF="distributing-the-crawler-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html4857"
  HREF="distributing-the-crawler-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html4851"
  HREF="crawling-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html4845"
  HREF="crawling-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html4853"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html4855"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html4858"
  HREF="distributing-the-crawler-1.html">Distributing the crawler</A>
<B> Up:</B> <A NAME="tex2html4852"
  HREF="crawling-1.html">Crawling</A>
<B> Previous:</B> <A NAME="tex2html4846"
  HREF="crawling-1.html">Crawling</A>
 &nbsp; <B>  <A NAME="tex2html4854"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html4856"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION002521000000000000000"></A>
<A NAME="sec:crawlarch"></A> <A NAME="p:crawlarch"></A>
<BR>
Crawler architecture
</H2> 

<P>
The simple scheme outlined above for crawling demands several
modules that fit together as shown in Figure <A HREF="#fig:fig:crawlarch">20.1</A> .

<OL>
<LI>The URL frontier, containing URLs yet to be fetched in the
current crawl (in the case of continuous crawling, a URL may have
been fetched previously but is back in the frontier for
re-fetching). We describe this further in Section <A HREF="the-url-frontier-1.html#sec:frontier">20.2.3</A> .
</LI>
<LI>A <I>DNS resolution</I> module that determines the web
server from which to fetch the page specified by a URL. We describe this
further in Section <A HREF="dns-resolution-1.html#sec:dns">20.2.2</A> .
</LI>
<LI>A fetch module that uses the http protocol to retrieve the web page at a URL.
</LI>
<LI>A parsing module that extracts the text and set of links from a
fetched web page.
</LI>
<LI>A duplicate elimination module that determines whether an extracted link is
already in the URL frontier or has recently been fetched.
</LI>
</OL>

<P>

<DIV ALIGN="CENTER"><A NAME="fig:fig:crawlarch"></A><A NAME="p:fig:crawlarch"></A><A NAME="31349"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 20.1:</STRONG>
The basic crawler architecture.</CAPTION>
<TR><TD><IMG
 WIDTH="441" HEIGHT="280" BORDER="0"
 SRC="img1882.png"
 ALT="\begin{figure}\begin{picture}(700,200)
\put(20,40){\framebox{(}20,120){www}}
\pu...
...0){\vector(0,1){15}}
\put(255,125){\vector(0,-1){15}}
\end{picture}
\end{figure}"></TD></TR>
</TABLE>
</DIV>

<P>
Crawling is performed by anywhere from one to potentially
hundreds of threads, each of which loops through the logical cycle
in Figure <A HREF="#fig:fig:crawlarch">20.1</A> . These threads may be run in a single
process, or be partitioned amongst multiple processes running at
different nodes of a distributed system. We begin by assuming that
the URL frontier is in place and non-empty and defer our
description of the implementation of the URL frontier to Section <A HREF="the-url-frontier-1.html#sec:frontier">20.2.3</A> .  We follow the progress of a single URL through the cycle of being fetched, passing through various checks and filters, then finally (for continuous crawling) being returned to the URL frontier.

<P>
A crawler thread begins by taking a URL from the frontier and
fetching the web page at that URL, generally using the http
protocol. The fetched page is then written into a temporary store,
where a number of operations are performed on it. Next, the page is parsed and the text as well as the links in it are extracted. The text (with any tag information - e.g., terms in
boldface) is passed on to the indexer. Link information including
anchor text is also passed on to the indexer for use in ranking in
ways that are described in Chapter <A HREF="link-analysis-1.html#ch:link">21</A> . In addition, each extracted link
goes through a series of tests to determine whether the link should
be added to the URL frontier.

<P>
First, the thread tests whether a web page with the same content has already been seen at another URL. The simplest implementation for this would use a simple fingerprint such as a checksum (placed in a store labeled "Doc FP's" in Figure <A HREF="#fig:fig:crawlarch">20.1</A> ). A more sophisticated test would use shingles instead of fingerprints, as described in Chapter <A HREF="web-search-basics-1.html#ch:webchar">19</A> .

<P>
Next, a <I>URL filter</I> is used to determine whether the
extracted URL should be excluded from the frontier based on one of
several tests. For instance, the crawl may seek to exclude certain
domains (say, all .com URLs) - in this case the test would simply
filter out the URL if it were from the .com domain. A similar test
could be inclusive rather than exclusive. Many hosts on the Web
place certain portions of their websites off-limits to crawling,
under a standard known as the <A NAME="31359"></A> <I>Robots Exclusion Protocol</I> , except for the robot called ``searchengine''.

<P>
<PRE>
User-agent: *
Disallow: /yoursite/temp/

User-agent: searchengine
Disallow:
</PRE>

<P>
The robots.txt file must be fetched from a website in order to test whether the URL under consideration passes the robot restrictions, and can therefore be added to the URL frontier. Rather than fetch it afresh for testing on each URL to be added to the frontier, a <A NAME="31363"></A>cache can be used to obtain a recently fetched copy of the file for the host.  This is especially important since many of the links extracted from a page fall within the host from which the page was fetched and therefore can be tested against the host's robots.txt file. Thus, by performing the filtering during the link extraction process, we would have especially high locality in the stream of hosts that we need to test for robots.txt files, leading to high cache hit rates. Unfortunately, this runs afoul of webmasters' politeness expectations. A URL (particularly one referring to a low-quality or rarely changing document) may be in the frontier for days or even weeks. If we were to perform the robots filtering <I>before</I> adding such a URL to the frontier, its robots.txt file could have changed by the time the URL is dequeued from the frontier and fetched. We must consequently perform robots-filtering immediately before attempting to fetch a web page. As it turns out, maintaining a cache of robots.txt files is still highly effective; there is sufficient locality even in the stream of URLs dequeued from the URL frontier.

<P>
Next, a URL should be <A NAME="31365"></A> <I>normalized</I>  in the following sense: often the HTML encoding of a link from a web page <IMG
 WIDTH="13" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img181.png"
 ALT="$p$"> indicates the target of that link relative to the page <IMG
 WIDTH="13" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img181.png"
 ALT="$p$">. Thus, there is a relative link encoded thus in the HTML of the page <TT><A NAME="tex2html206"
  HREF="en.wikipedia.org/wiki/Main_Page">en.wikipedia.org/wiki/Main_Page</A></TT>:
<BLOCKQUOTE>
<TT><A NAME="tex2html207"
  HREF="&lt;a href="/wiki/Wikipedia:General_disclaimer"">&lt;a href="/wiki/Wikipedia:General_disclaimer"</A><A NAME="tex2html208"
  HREF="title="Wikipedia:General">title="Wikipedia:General</A><A NAME="tex2html209"
  HREF="disclaimer"&gt;Disclaimers&lt;/a&gt;">disclaimer"&gt;Disclaimers&lt;/a&gt;</A></TT>
</BLOCKQUOTE>
points to the URL
<TT><A NAME="tex2html210"
  HREF="http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer">http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer</A></TT>.

<P>
Finally, the URL is checked for duplicate elimination: if the URL is already in the frontier or (in the case of a non-continuous crawl) already crawled, we do not add it to the frontier. When the URL is added to the frontier, it is assigned a priority based on which it is eventually removed from the frontier for fetching. The details of this priority queuing are in Section <A HREF="the-url-frontier-1.html#sec:frontier">20.2.3</A> .

<P>
Certain housekeeping tasks are typically performed by a dedicated
thread. This thread is generally quiescent except that it wakes up
once every few seconds to log crawl progress statistics (URLs
crawled, frontier size, etc.), decide whether to terminate the
crawl, or (once every few hours of crawling) checkpoint the crawl.
In checkpointing, a snapshot of the crawler's state (say, the URL
frontier) is committed to disk. In the event of a catastrophic
crawler failure, the crawl is restarted from the most recent
checkpoint.

<P>
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><A NAME="tex2html4859"
  HREF="distributing-the-crawler-1.html">Distributing the crawler</A>
</UL>
<!--End of Table of Child-Links-->
<HR>
<!--Navigation Panel-->
<A NAME="tex2html4857"
  HREF="distributing-the-crawler-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html4851"
  HREF="crawling-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html4845"
  HREF="crawling-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html4853"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html4855"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html4858"
  HREF="distributing-the-crawler-1.html">Distributing the crawler</A>
<B> Up:</B> <A NAME="tex2html4852"
  HREF="crawling-1.html">Crawling</A>
<B> Previous:</B> <A NAME="tex2html4846"
  HREF="crawling-1.html">Crawling</A>
 &nbsp; <B>  <A NAME="tex2html4854"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html4856"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
