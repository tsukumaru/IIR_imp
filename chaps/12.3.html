
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Language modeling versus other approaches in IR</TITLE>
<META NAME="description" CONTENT="Language modeling versus other approaches in IR">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="extended-language-modeling-approaches-1.html">
<LINK REL="previous" HREF="the-query-likelihood-model-1.html">
<LINK REL="up" HREF="language-models-for-information-retrieval-1.html">
<LINK REL="next" HREF="extended-language-modeling-approaches-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html3370"
  HREF="extended-language-modeling-approaches-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3364"
  HREF="language-models-for-information-retrieval-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3358"
  HREF="ponte-and-crofts-experiments-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3366"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3368"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3371"
  HREF="extended-language-modeling-approaches-1.html">Extended language modeling approaches</A>
<B> Up:</B> <A NAME="tex2html3365"
  HREF="language-models-for-information-retrieval-1.html">Language models for information</A>
<B> Previous:</B> <A NAME="tex2html3359"
  HREF="ponte-and-crofts-experiments-1.html">Ponte and Croft's Experiments</A>
 &nbsp; <B>  <A NAME="tex2html3367"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3369"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION001730000000000000000"></A><A NAME="sec:lm-vs-other"></A> <A NAME="p:lm-vs-other"></A>
<BR>
Language modeling versus other approaches in IR
</H1> 

<P>
The language modeling approach provides a novel way of looking at the
problem of text retrieval, which links it with a lot of recent work in
speech and language processing.
As <A
 HREF="bibliography-1.html#ponte98lm">Ponte and Croft (1998)</A> emphasize, the language modeling approach to IR
provides a different approach to scoring matches between queries and
documents, and the hope is that the probabilistic language modeling
foundation improves the weights that are used, and hence the performance
of the model.  The major issue is estimation of the document model, such
as choices of how to smooth it effectively.  The model has achieved very good
retrieval results.  Compared to other probabilistic approaches, such
as the BIM
from Chapter <A HREF="probabilistic-information-retrieval-1.html#ch:probir">11</A> , the main difference initially appears to be
that the LM approach does away with explicitly modeling relevance
(whereas this is 
the central variable evaluated in the BIM approach).  But this
may not be the correct way to think about things, as some of the
papers in Section <A HREF="references-and-further-reading-12.html#sec:lmir-refs">12.5</A>  further discuss.  The LM approach
assumes that documents and expressions of information needs are
objects of the same type, and assesses their match by importing the
tools and methods of language modeling from speech and <A NAME="15493"></A>natural language
processing.  The resulting model is mathematically precise, conceptually
simple, computationally tractable, and intuitively appealing. 
This seems similar to the situation with XML retrieval
(Chapter <A HREF="xml-retrieval-1.html#ch:xml">10</A> ): there the approaches that
assume <A NAME="15495"></A>queries and documents are objects of the same type
are also among the most successful.

<P>
On the other hand, like all IR models, you can also raise objections to
the model.  The assumption of equivalence between document and information
need representation is unrealistic.  Current LM approaches use very
simple models of language, usually unigram models.  Without an explicit
notion of relevance, relevance feedback is difficult to integrate into
the model, as are user preferences.
It also seems necessary to move beyond a unigram model to accommodate
notions of phrase or passage matching or Boolean retrieval operators.  Subsequent
work in the LM approach has looked at addressing some of these
concerns, including putting relevance back into the model and allowing a
language mismatch between the query language and the document language.

<P>
The model has significant relations to traditional tf-idf models.  Term
frequency is directly 
represented
in tf-idf models, and much recent work has
recognized the importance of document length normalization.  The effect
of doing a mixture of document generation probability with collection
generation probability is a little like idf: terms rare in the general
collection but common in some documents will have a greater influence on
the ranking of documents.  In most concrete realizations, the models
share treating terms as if they were independent.  On the other hand, the
intuitions are probabilistic rather than geometric, the mathematical
models are more principled rather than heuristic, and the details of how
statistics like term frequency and document length are used differ.
If you are concerned mainly with performance numbers, recent work has
shown the LM approach to be very effective in retrieval experiments,
beating tf-idf and BM25 weights. Nevertheless, there is perhaps still 
insufficient evidence that its performance so greatly exceeds that of
a well-tuned traditional  vector space retrieval system as to justify
changing an existing implementation.

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html3370"
  HREF="extended-language-modeling-approaches-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3364"
  HREF="language-models-for-information-retrieval-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3358"
  HREF="ponte-and-crofts-experiments-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3366"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3368"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3371"
  HREF="extended-language-modeling-approaches-1.html">Extended language modeling approaches</A>
<B> Up:</B> <A NAME="tex2html3365"
  HREF="language-models-for-information-retrieval-1.html">Language models for information</A>
<B> Previous:</B> <A NAME="tex2html3359"
  HREF="ponte-and-crofts-experiments-1.html">Ponte and Croft's Experiments</A>
 &nbsp; <B>  <A NAME="tex2html3367"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3369"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
