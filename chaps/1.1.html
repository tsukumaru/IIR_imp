
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>An example information retrieval problem</TITLE>
<META NAME="description" CONTENT="An example information retrieval problem">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="a-first-take-at-building-an-inverted-index-1.html">
<LINK REL="previous" HREF="boolean-retrieval-1.html">
<LINK REL="up" HREF="boolean-retrieval-1.html">
<LINK REL="next" HREF="a-first-take-at-building-an-inverted-index-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html936"
  HREF="a-first-take-at-building-an-inverted-index-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html930"
  HREF="boolean-retrieval-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html924"
  HREF="boolean-retrieval-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html932"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html934"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html937"
  HREF="a-first-take-at-building-an-inverted-index-1.html">A first take at</A>
<B> Up:</B> <A NAME="tex2html931"
  HREF="boolean-retrieval-1.html">Boolean retrieval</A>
<B> Previous:</B> <A NAME="tex2html925"
  HREF="boolean-retrieval-1.html">Boolean retrieval</A>
 &nbsp; <B>  <A NAME="tex2html933"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html935"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION00610000000000000000"></A><A NAME="sec:example-problem"></A> <A NAME="p:example-problem"></A>
<BR>
An example information retrieval problem
</H1> 

<P>
A fat book which many people own is Shakespeare's Collected Works.
Suppose you wanted to determine which plays of Shakespeare contain the
words Brutus AND Caesar and NOT
Calpurnia.  One way
to do that is to start at the beginning and to read through all the
text, noting for each play whether it contains Brutus and
Caesar and excluding it from consideration if it contains
Calpurnia.
The simplest form of document retrieval is for a computer to do this
sort of linear scan
through documents.  This process is commonly referred to as
<A NAME="842"></A> <I>grepping</I>  through text, after the Unix command grep, which
performs this process.  Grepping through text can be a very effective
process, especially given the speed of modern computers, and often
allows useful possibilities for <A NAME="845"></A>wildcard
pattern matching through the
use of <A NAME="846"></A>  .
With modern computers, for simple querying of modest
collections (the size of Shakespeare's Collected Works is a bit under
one million words of text in total), you really need nothing more.

<P>
But for many purposes, you do need more:

<OL>
<LI>To process large document collections quickly. The amount of
  online data has grown at least
as quickly as the speed of computers, and we would now like to be able
to search collections that total in the order of billions to trillions
of words.
</LI>
<LI>To allow more flexible matching operations.  For example, it is
  impractical to perform the query
  Romans NEAR countrymen with grep, where
  NEAR might be defined as ``within 5 words'' or ``within the
  same sentence''.
</LI>
<LI>To allow ranked retrieval: in many cases you want the best answer
  to an information need among many documents that contain certain words.
</LI>
</OL>

<P>
The way to avoid linearly scanning the texts for each query is
to <A NAME="855"></A> <I>index</I>  the documents in advance.  Let us stick with
Shakespeare's Collected Works, and
use it to introduce the basics of the Boolean
retrieval model.   Suppose we record for each
document - here a play of Shakespeare's - whether it contains each word
out of all the words Shakespeare used (Shakespeare used about 32,000
different words).  The result is a binary term-document <A NAME="p:incidencematrix"></A> 
<A NAME="858"></A> <I>incidence matrix</I> , as in Figure <A HREF="#fig:termdoc">1.1</A> .  <A NAME="861"></A> <I>Terms</I>  are the indexed units (further discussed in Section <A HREF="determining-the-vocabulary-of-terms-1.html#sec:dictionary-terms">2.2</A> ); they are usually words, and for the moment you can think of them as words, but the information retrieval
literature normally speaks of terms because some of them, such as perhaps I-9
or Hong Kong are not usually thought of as words.
Now, depending on whether we look at the
matrix rows or columns, we can have a vector for each term, which shows
the documents it appears in, or a vector for each document, showing the
terms that occur in it.<A NAME="tex2html5"
  HREF="footnode.html#foot866"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A>
<P>

<DIV ALIGN="CENTER">

<P><A NAME="fig:termdoc"></A><A NAME="p:termdoc"></A></P><IMG
 WIDTH="564" HEIGHT="258" BORDER="0"
 SRC="img38.png"
 ALT="\begin{figure}
% latex2html id marker 867
\begin{tabular}{@{}lccccccc@{}}
&amp; Ant...
...y in column~$d$\ contains the word in row~$t$, and is 0 otherwise.}
\end{figure}">
</DIV>

<P>
To answer the query Brutus AND Caesar AND
NOT
Calpurnia, we take the vectors for Brutus, Caesar
and Calpurnia, complement the last, and then do a bitwise AND:
<BLOCKQUOTE>
110100 AND 110111 AND 101111 = 100100

</BLOCKQUOTE>
The answers for this query are thus Antony and Cleopatra
and Hamlet (Figure <A HREF="#fig:shakeanswer">1.2</A> ).

<P>
The
<A NAME="894"></A> <I>Boolean retrieval model</I> 
is a model for information
retrieval in which we can pose any query which is in the form of a Boolean expression of terms,
that is, in which terms are combined with
the operators and, or, and not.
The model views each document as just a set of words.

<P>

<DIV ALIGN="CENTER"><A NAME="fig:shakeanswer"></A><A NAME="p:shakeanswer"></A><A NAME="1533"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure:</STRONG>
Results from Shakespeare for the query
Brutus AND Caesar AND NOT
Calpurnia.</CAPTION>
<TR><TD><IMG
 WIDTH="492" HEIGHT="163" BORDER="0"
 SRC="img39.png"
 ALT="\begin{figure}\parbox{\textwidth}{
\emph{Antony and Cleopatra, Act III, Scene ii...
...he \\
\hspace*{1.5in} &amp; Capitol; Brutus killed me.
\end{tabular}}\end{figure}"></TD></TR>
</TABLE>
</DIV>

<P>
Let us now consider a more realistic scenario, simultaneously using the
opportunity to introduce some terminology and notation.
Suppose we have <!-- MATH
 $N = 1\mbox{ million}$
 -->
<IMG
 WIDTH="102" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img40.png"
 ALT="$N = 1\mbox{ million}$"><A NAME="N-notation"></A> documents.  By
<A NAME="928"></A> <I>documents</I> 
we mean whatever units we have decided to build a retrieval system
over.  They might be individual memos or chapters of a book (see
Section&nbsp;<A HREF="choosing-a-document-unit-1.html#sec:documentunit">2.1.2</A> (page&nbsp;<A HREF="choosing-a-document-unit-1.html#p:documentunit"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/crossref.png"></A>) for further discussion).
We will refer to the group of documents over which we
perform retrieval as the (document) <A NAME="1534"></A><A NAME="933"></A> <I>collection</I> .  It is sometimes
also referred to as a <A NAME="935"></A> <I>corpus</I>  (a <I>body</I> of texts).
Suppose each document is about 1000 words long (2-3 book pages).
If we assume an average of 6 bytes per word including spaces and
punctuation, then this is a document collection about 6&nbsp;GB in size.
Typically, there might be about <IMG
 WIDTH="94" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img41.png"
 ALT="$M = 500{,}000$"><A NAME="M-notation"></A>distinct terms in these
documents.  There is nothing special about the numbers
we have chosen, and they might vary by an order of magnitude or more, but they
give us some idea of the dimensions of the kinds of problems we need to
handle.  We will discuss and model these size assumptions in
Section&nbsp;<A HREF="statistical-properties-of-terms-in-information-retrieval-1.html#sec:statproperties">5.1</A> (page&nbsp;<A HREF="statistical-properties-of-terms-in-information-retrieval-1.html#p:statproperties"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/crossref.png"></A>).

<P>
Our goal is to develop a system to address
the <A NAME="942"></A> <I>ad hoc retrieval</I>  task.  This is the most standard IR task.
In it, a system aims to provide documents from within the collection that are
relevant to an arbitrary user information need, communicated to the
system by means of a one-off, user-initiated query.  An
<A NAME="944"></A> <I>information need</I>  is the topic about which the user desires to
know more, and is differentiated from a <A NAME="946"></A> <I>query</I> , which is what
the user conveys to the computer in an attempt to communicate the
information need.
A document is <A NAME="948"></A> <I>relevant</I>  if it is
one that the user perceives as containing information of value with
respect to their personal information need.
Our example above was rather artificial in that the
information need was defined in terms of particular words, whereas
usually a user is interested in a topic like ``pipeline leaks'' and
would like to find relevant documents regardless of whether they
precisely use those words or express the concept with other words such
as pipeline rupture.
To assess the <A NAME="951"></A> <I>effectiveness</I>  of an IR system (i.e., the
quality of its search results), a user will usually want to know two key
statistics about the system's returned results for a query:
<DL>
<DT></DT>
<DD><A NAME="954"></A> <I>Precision</I> : What fraction of
the returned results are relevant to the information need?
</DD>
<DT></DT>
<DD><A NAME="956"></A> <I>Recall</I> : What fraction of the relevant documents in
  the collection were returned by the system?
</DD>
</DL>
Detailed discussion of relevance and evaluation measures including
precision and recall is found in Chapter <A HREF="evaluation-in-information-retrieval-1.html#ch:evaluation">8</A> .

<P>
We now cannot build a term-document matrix in a naive way.
A <!-- MATH
 $500\mathrm{K} \times 1\mathrm{M}$
 -->
<IMG
 WIDTH="82" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img42.png"
 ALT="$500\mathrm{K} \times 1\mathrm{M}$"> matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory.
But the crucial observation is that the matrix is extremely sparse, that
is, it has few non-zero entries.  Because each document is 1000 words
long, the matrix has no more than one
billion 1's, so a minimum of 99.8% of the cells are zero.
A much better representation is to record only the things
that do occur, that is, the 1 positions.

<P>
<A NAME="p:invertedindex"></A> 
This idea is central to the first major concept in information
retrieval, the <A NAME="963"></A> <I>inverted index</I> .  The name is actually redundant:
an index always maps back from terms to the parts of a
document where they occur. Nevertheless, <I>inverted index</I>, or
sometimes <A NAME="1535"></A> <I>inverted file</I> , has
become the standard term
in information retrieval.<A NAME="tex2html7"
  HREF="footnode.html#foot1536"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A>The basic idea of an inverted index is shown
in Figure <A HREF="#fig:invertedindex-picture">1.3</A> .  We keep a <A NAME="975"></A> <I>dictionary</I> 
of terms (sometimes also referred to as a <A NAME="977"></A> <I>vocabulary</I>  or <A NAME="979"></A> <I>lexicon</I> ; in this book, we use <I>dictionary</I> for the data structure and <I>vocabulary</I> for the set of terms).
Then for each term, we have a list that records which documents the
term occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called
a <A NAME="983"></A> <I>posting</I> .<A NAME="tex2html8"
  HREF="footnode.html#foot985"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A>The list is then called a <A NAME="986"></A> <I>postings list</I> 
(or <A NAME="1537"></A>  ),
and all the postings
lists taken together are referred to as the
<A NAME="990"></A> <I>postings</I> .
The dictionary in Figure <A HREF="#fig:invertedindex-picture">1.3</A>  has been sorted
alphabetically and each postings list is
sorted by document ID.  We will see why this is useful in
Section <A HREF="processing-boolean-queries-1.html#sec:postingsintersection">1.3</A> , below, but later we will
also consider
alternatives to doing this (Section <A HREF="impact-ordering-1.html#sec:impactordered">7.1.5</A> ).

<P>

<DIV ALIGN="CENTER">

<P><A NAME="fig:invertedindex-picture"></A><A NAME="p:invertedindex-picture"></A></P><IMG
 WIDTH="556" HEIGHT="243" BORDER="0"
 SRC="img43.png"
 ALT="\begin{figure}
% latex2html id marker 995
\begin{tabular}{\vert c\vert c\vert r\...
...ry, with pointers to each postings list, which is stored
on disk.}
\end{figure}">
</DIV>

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html936"
  HREF="a-first-take-at-building-an-inverted-index-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html930"
  HREF="boolean-retrieval-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html924"
  HREF="boolean-retrieval-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html932"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html934"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html937"
  HREF="a-first-take-at-building-an-inverted-index-1.html">A first take at</A>
<B> Up:</B> <A NAME="tex2html931"
  HREF="boolean-retrieval-1.html">Boolean retrieval</A>
<B> Previous:</B> <A NAME="tex2html925"
  HREF="boolean-retrieval-1.html">Boolean retrieval</A>
 &nbsp; <B>  <A NAME="tex2html933"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html935"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
