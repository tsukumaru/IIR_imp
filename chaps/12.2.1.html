
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Using query likelihood language models in IR</TITLE>
<META NAME="description" CONTENT="Using query likelihood language models in IR">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="estimating-the-query-generation-probability-1.html">
<LINK REL="previous" HREF="the-query-likelihood-model-1.html">
<LINK REL="up" HREF="the-query-likelihood-model-1.html">
<LINK REL="next" HREF="estimating-the-query-generation-probability-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html3328"
  HREF="estimating-the-query-generation-probability-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3322"
  HREF="the-query-likelihood-model-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3316"
  HREF="the-query-likelihood-model-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3324"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3326"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3329"
  HREF="estimating-the-query-generation-probability-1.html">Estimating the query generation</A>
<B> Up:</B> <A NAME="tex2html3323"
  HREF="the-query-likelihood-model-1.html">The query likelihood model</A>
<B> Previous:</B> <A NAME="tex2html3317"
  HREF="the-query-likelihood-model-1.html">The query likelihood model</A>
 &nbsp; <B>  <A NAME="tex2html3325"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3327"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION001721000000000000000"></A>
<A NAME="sec:querylikelihoodmodel"></A> <A NAME="p:querylikelihoodmodel"></A>
<BR>
Using query likelihood language models in IR
</H2> 

<P>
Language modeling is a quite general formal approach to IR, with many
variant realizations. The original and basic method for using language
models in IR is the <A NAME="15333"></A> <I>query likelihood model</I> .  In it, we
construct from 
each document <IMG
 WIDTH="12" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img354.png"
 ALT="$d$"> in the collection a language model <IMG
 WIDTH="27" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img789.png"
 ALT="$M_d$">.  Our goal is
to rank documents by <IMG
 WIDTH="49" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img812.png"
 ALT="$P(d\vert q)$">, where the probability of a document is
interpreted as the likelihood that it is relevant to the query.  Using
Bayes rule (as introduced in probirsec), we have:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
P(d|q) = P(q|d)P(d)/P(q)
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="185" HEIGHT="28" BORDER="0"
 SRC="img813.png"
 ALT="\begin{displaymath}
P(d\vert q) = P(q\vert d)P(d)/P(q)
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(98)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
<IMG
 WIDTH="35" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img814.png"
 ALT="$P(q)$"> is the same for all documents, and so can be ignored.
The prior probability of a document <IMG
 WIDTH="36" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img815.png"
 ALT="$P(d)$"> is often treated as uniform
across all <IMG
 WIDTH="12" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img354.png"
 ALT="$d$"> and so it can also be 
ignored, but we could implement a genuine prior which could include
criteria like authority, length, genre, newness, and number of previous
people who have read the document.  But, given these simplifications, we
return results ranked by simply <IMG
 WIDTH="49" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img816.png"
 ALT="$P(q\vert d)$">, the probability of the query
<IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img161.png"
 ALT="$q$"> under the language model derived from <IMG
 WIDTH="12" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img354.png"
 ALT="$d$">.
The Language Modeling approach thus attempts to model the query generation
process: Documents are ranked by the probability that a query would be
observed as a random sample from the respective document model.

<P>
The most common way to do this is using the multinomial unigram language
model, which is equivalent to a multinomial Naive Bayes
model (page <A HREF="the-bernoulli-model-1.html#p:twomodels">13.3</A> ), where the
documents are the classes, each treated in the estimation as a separate
``language''.  Under this model, we have that:
<A NAME="p:multinomial"></A> 
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
P(q|M_d) = K_q \prod_{t \in V} P(t|M_d)^{\termf_{t,d}}
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="p:multinomialfactor"></A><A NAME="multinomialfactor"></A><A NAME="eqn:multinomial"></A><IMG
 WIDTH="221" HEIGHT="50" BORDER="0"
 SRC="img817.png"
 ALT="\begin{displaymath}P(q\vert M_d) = K_q \prod_{t \in V} P(t\vert M_d)^{\termf_{t,d}}
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(99)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
where, again <!-- MATH
 $K_q = L_d!/(\termf_{t_1,d}!\termf_{t_2,d}!\cdots
\termf_{t_M,d}!)$
 -->
<IMG
 WIDTH="225" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img818.png"
 ALT="$K_q = L_d!/(\termf_{t_1,d}!\termf_{t_2,d}!\cdots
\termf_{t_M,d}!)$"> is the multinomial coefficient for the query <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img161.png"
 ALT="$q$">,
which we will henceforth ignore, since it is a constant for a
particular query.

<P>
For retrieval based on a language model (henceforth <A NAME="15350"></A> <I>LM</I> ), we 
treat the generation of queries as a random process.
The approach is to 

<OL>
<LI>Infer a LM for each document.
</LI>
<LI>Estimate <IMG
 WIDTH="68" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img819.png"
 ALT="$P(q\vert M_{d_i})$">, the probability of generating the query
  according to each of these document models.
</LI>
<LI>Rank the documents according to these probabilities.
</LI>
</OL>
The intuition of the basic model is that the user has a prototype document
in mind, and
generates a query based on words that appear in this document.
Often, users
have a reasonable idea of terms that are likely to occur in documents of
interest and they will choose query terms that distinguish these
documents from others in the collection.<A NAME="tex2html117"
  HREF="footnode.html#foot15596"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A>Collection statistics are an integral part of the language model, rather
than being used heuristically as in many other approaches.

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html3328"
  HREF="estimating-the-query-generation-probability-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3322"
  HREF="the-query-likelihood-model-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3316"
  HREF="the-query-likelihood-model-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3324"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3326"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3329"
  HREF="estimating-the-query-generation-probability-1.html">Estimating the query generation</A>
<B> Up:</B> <A NAME="tex2html3323"
  HREF="the-query-likelihood-model-1.html">The query likelihood model</A>
<B> Previous:</B> <A NAME="tex2html3317"
  HREF="the-query-likelihood-model-1.html">The query likelihood model</A>
 &nbsp; <B>  <A NAME="tex2html3325"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3327"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
