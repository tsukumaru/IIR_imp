
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Finite automata and language models</TITLE>
<META NAME="description" CONTENT="Finite automata and language models">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="types-of-language-models-1.html">
<LINK REL="previous" HREF="language-models-1.html">
<LINK REL="up" HREF="language-models-1.html">
<LINK REL="next" HREF="types-of-language-models-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html3271"
  HREF="types-of-language-models-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3265"
  HREF="language-models-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3259"
  HREF="language-models-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3267"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3269"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3272"
  HREF="types-of-language-models-1.html">Types of language models</A>
<B> Up:</B> <A NAME="tex2html3266"
  HREF="language-models-1.html">Language models</A>
<B> Previous:</B> <A NAME="tex2html3260"
  HREF="language-models-1.html">Language models</A>
 &nbsp; <B>  <A NAME="tex2html3268"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3270"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION001711000000000000000">
Finite automata and language models</A>
</H2>

<P>

<DIV ALIGN="CENTER">

<P><A NAME="fig:fsm"></A><A NAME="p:fsm"></A></P><IMG
 WIDTH="555" HEIGHT="195" BORDER="0"
 SRC="img791.png"
 ALT="\begin{figure}
% latex2html id marker 15157
\begin{pspicture}(0,0)(5,3)
\rput(1,...
...omaton and a double circle indicates a (possible) finishing
state.}\end{figure}">
</DIV>

<P>
<A NAME="p:generativemodel"></A> 
What do we mean by a document model generating a query?  A traditional
<A NAME="15181"></A> <I>generative model</I>  of a language, of the kind familiar from formal
language theory, can be used either to recognize or to generate strings.
For example, the finite automaton shown in Figure <A HREF="#fig:fsm">12.1</A>  can generate
strings that include the examples shown.  The full set of strings that
can be generated is called the <A NAME="15184"></A> <I>language</I>  of the
automaton.<A NAME="tex2html114"
  HREF="footnode.html#foot15186"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A>
<P>

<DIV ALIGN="CENTER">

<P><A NAME="fig:fsm2"></A><A NAME="p:fsm2"></A></P><IMG
 WIDTH="555" HEIGHT="201" BORDER="0"
 SRC="img792.png"
 ALT="\begin{figure}
% latex2html id marker 15187
\begin{tabular}[b]{c}
\begin{pspictu...
... show a partial specification of the state emission
probabilities.}\end{figure}">
</DIV>

<P>
If instead each node has a probability distribution over generating
different terms, we have a language model.  The notion of a language model is inherently probabilistic.  A <A NAME="15211"></A> <I>language model</I> 
is a function that puts a 
probability measure over strings drawn from some vocabulary.  That is, for 
a language model <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img186.png"
 ALT="$M$"> over an alphabet <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img793.png"
 ALT="$\Sigma$">:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\sum_{s \in \Sigma^{*}} P(s) = 1
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="lang-model-eq"></A><IMG
 WIDTH="91" HEIGHT="43" BORDER="0"
 SRC="img794.png"
 ALT="\begin{displaymath}
\sum_{s \in \Sigma^{*}} P(s) = 1
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(90)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
One simple kind of language model is equivalent to a probabilistic
finite automaton consisting of just a single node
with a single probability distribution over producing different terms,
so that <!-- MATH
 $\sum_{t \in V} P(t) = 1$
 -->
<IMG
 WIDTH="102" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img795.png"
 ALT="$\sum_{t \in V} P(t) = 1$">, as
shown in Figure <A HREF="#fig:fsm2">12.2</A> .  After generating each word, we decide whether
to stop or to loop around and then produce another word, and so the
model also requires a probability of stopping in the
finishing state.  Such a model places a probability distribution
over any sequence of words.  By construction, it also provides a model
for generating text according to its distribution.  

<P>
<B>Worked example.</B> <A NAME="m1probability"></A>To find the
probability of a word sequence, we just multiply the probabilities
which the model gives to each word in the sequence, together with the
probability of continuing or stopping after producing each word.  For example,
<BR>
<DIV ALIGN="CENTER">

<!-- MATH
 \begin{eqnarray}
P(\mbox{frog said that toad likes frog}) & = &
(0.01 \times 0.03 \times 0.04 \times 0.01 \times 0.02 \times 0.01) \\
& & \times (0.8 \times 0.8 \times 0.8 \times 0.8 \times 0.8 \times 0.8 \times 0.2) \\
& \approx & 0.000000000001573
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="226" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img796.png"
 ALT="$\displaystyle P(\mbox{frog said that toad likes frog})$"></TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img313.png"
 ALT="$\textstyle =$"></TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="282" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img797.png"
 ALT="$\displaystyle (0.01 \times 0.03 \times 0.04 \times 0.01 \times 0.02 \times 0.01)$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(91)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD>&nbsp;</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="287" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img798.png"
 ALT="$\displaystyle \times (0.8 \times 0.8 \times 0.8 \times 0.8 \times 0.8 \times 0.8 \times 0.2)$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(92)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img799.png"
 ALT="$\textstyle \approx$"></TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="135" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img800.png"
 ALT="$\displaystyle 0.000000000001573$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(93)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
As you can see, the probability of a particular string/document, is usually a
very small number!  Here we stopped after generating <I>frog</I> the
second time. The
first line of numbers are the term emission probabilities, and the
second line gives the probability of continuing or stopping after
generating each word.  An explicit
stop probability is needed for a finite automaton to be a well-formed
language model according to Equation&nbsp;<A HREF="#lang-model-eq">90</A>. Nevertheless, most
of the time, we will omit to include <SMALL>STOP</SMALL> and <!-- MATH
 $(1-
\mbox{\textsc{stop}})$
 -->
<IMG
 WIDTH="80" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img801.png"
 ALT="$(1-
\mbox{\textsc{stop}})$"> probabilities (as do most
other authors).  To compare two models for a data set, we can
calculate their <A NAME="15228"></A> <I>likelihood ratio</I> , which results from simply dividing
the probability of the data according to one model by the probability
of the data according to the other model. 
Providing that the stop probability is
fixed, its inclusion will not alter the likelihood ratio that results
from comparing the likelihood of two language models generating
a string.  Hence, it will not alter the ranking of documents.<A NAME="tex2html115"
  HREF="footnode.html#foot15230"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A> 
Nevertheless, formally, the numbers will no longer truly be
probabilities, but only proportional to probabilities.  See
Exercise <A HREF="multinomial-distributions-over-words-1.html#ex:nostop-lr">12.1.3</A> . 
<B>End worked example.</B>

<P>

<DIV ALIGN="CENTER"><A NAME="fig:lm1"></A><A NAME="p:lm1"></A><A NAME="15243"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 12.3:</STRONG>
Partial specification of two unigram language
  models.</CAPTION>
<TR><TD><IMG
 WIDTH="233" HEIGHT="231" BORDER="0"
 SRC="img802.png"
 ALT="\begin{figure}\begin{tabular}{\vert ll\vert ll\vert}
\hline
\multicolumn{2}{\ver...
...2 \\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\ \hline
\end{tabular}\par
\end{figure}"></TD></TR>
</TABLE>
</DIV>

<P>
<B>Worked example.</B> <A NAME="m1m2compare"></A>Suppose, now, that we have two language models <IMG
 WIDTH="27" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img803.png"
 ALT="$M_1$"> and <IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img804.png"
 ALT="$M_2$">, shown
partially in Figure <A HREF="#fig:lm1">12.3</A> .  Each gives a probability estimate to a
sequence of terms, as already illustrated  in m1probability.
The language model that
gives the higher probability to the sequence of terms is more likely to
have generated the term sequence.  This time, we will omit
<SMALL>STOP</SMALL> probabilities from our calculations.  
For the sequence shown, we get:
<BR>
<IMG
 WIDTH="404" HEIGHT="108" ALIGN="BOTTOM" BORDER="0"
 SRC="img805.png"
 ALT="\begin{example}
\begin{tabular}[t]{lllllllll}
$s$\ &amp; frog &amp; said &amp; that &amp; toad &amp;...
...column{5}{l}{$P(s\vert M_2) = 0.000000000000000384 $}
\end{tabular}\end{example}">
<BR>
and we see that <!-- MATH
 $P(s|M_1)> P(s|M_2)$
 -->
<IMG
 WIDTH="143" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img806.png"
 ALT="$P(s\vert M_1)&gt; P(s\vert M_2)$">.
We present the formulas here in terms of products of probabilities,
but, as is common in probabilistic applications, in practice it is
usually best to work with sums of log probabilities (cf. page <A HREF="naive-bayes-text-classification-1.html#p:use-log-probabilities">13.2</A> ).
<B>End worked example.</B>

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html3271"
  HREF="types-of-language-models-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3265"
  HREF="language-models-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3259"
  HREF="language-models-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3267"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3269"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3272"
  HREF="types-of-language-models-1.html">Types of language models</A>
<B> Up:</B> <A NAME="tex2html3266"
  HREF="language-models-1.html">Language models</A>
<B> Previous:</B> <A NAME="tex2html3260"
  HREF="language-models-1.html">Language models</A>
 &nbsp; <B>  <A NAME="tex2html3268"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3270"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
