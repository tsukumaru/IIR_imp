
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Evaluation of unranked retrieval sets</TITLE>
<META NAME="description" CONTENT="Evaluation of unranked retrieval sets">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="evaluation-of-ranked-retrieval-results-1.html">
<LINK REL="previous" HREF="standard-test-collections-1.html">
<LINK REL="up" HREF="evaluation-in-information-retrieval-1.html">
<LINK REL="next" HREF="evaluation-of-ranked-retrieval-results-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html2431"
  HREF="evaluation-of-ranked-retrieval-results-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html2425"
  HREF="evaluation-in-information-retrieval-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html2419"
  HREF="standard-test-collections-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html2427"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html2429"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html2432"
  HREF="evaluation-of-ranked-retrieval-results-1.html">Evaluation of ranked retrieval</A>
<B> Up:</B> <A NAME="tex2html2426"
  HREF="evaluation-in-information-retrieval-1.html">Evaluation in information retrieval</A>
<B> Previous:</B> <A NAME="tex2html2420"
  HREF="standard-test-collections-1.html">Standard test collections</A>
 &nbsp; <B>  <A NAME="tex2html2428"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html2430"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION001330000000000000000"></A>
<A NAME="sec:measuresperf"></A> <A NAME="p:measuresperf"></A> <A NAME="sec:unranked-evaluation"></A> <A NAME="p:unranked-evaluation"></A>
<BR>
Evaluation of unranked retrieval sets
</H1>  

<P>
Given these ingredients, how is system effectiveness measured?  The two
most frequent and basic measures for information retrieval effectiveness
are precision and recall.  These are first defined for the simple case
where an IR system returns a set of documents for a query.  We will see
later how to extend these notions to ranked retrieval situations.
<DL>
<DT></DT>
<DD><A NAME="10625"></A> <I>Precision</I>  (<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$"><A NAME="P-notation"></A>) is the fraction of retrieved documents that
  are relevant
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\mbox{Precision} = \frac{\#(\mbox{relevant items retrieved})}{\#(\mbox{retrieved items})}
= P(\mbox{relevant}|\mbox{retrieved})
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="449" HEIGHT="45" BORDER="0"
 SRC="img509.png"
 ALT="\begin{displaymath}
\mbox{Precision} = \frac{\char93 (\mbox{relevant items retri...
...x{retrieved items})}
= P(\mbox{relevant}\vert\mbox{retrieved})
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(36)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
</DD>
<DT></DT>
<DD><A NAME="10635"></A> <I>Recall</I>  (<IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img143.png"
 ALT="$R$"><A NAME="R-notation"></A>) is the fraction of relevant documents that
  are retrieved
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\mbox{Recall} = \frac{\#(\mbox{relevant items retrieved})}{\#(\mbox{relevant items})} =
 P(\mbox{retrieved}|\mbox{relevant})
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="428" HEIGHT="45" BORDER="0"
 SRC="img510.png"
 ALT="\begin{displaymath}
\mbox{Recall} = \frac{\char93 (\mbox{relevant items retrieve...
...x{relevant items})} =
P(\mbox{retrieved}\vert\mbox{relevant})
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(37)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
</DD>
</DL>
These notions can be made clear by examining the following contingency
table:
<BR>
<IMG
 WIDTH="459" HEIGHT="72" ALIGN="BOTTOM" BORDER="0"
 SRC="img511.png"
 ALT="\begin{example}
\begin{tabular}[t]{\vert l\vert l\vert l\vert}
\hline
&amp; Releva...
... false negatives (fn) &amp; true negatives (tn) \\ \hline
\end{tabular}\end{example}">
<BR>
Then:
<BR>
<DIV ALIGN="CENTER">

<!-- MATH
 \begin{eqnarray}
P &=& tp/(tp + fp) \\
R &=& tp/(tp + fn)
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img512.png"
 ALT="$\displaystyle P$"></TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img313.png"
 ALT="$\textstyle =$"></TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="95" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img513.png"
 ALT="$\displaystyle tp/(tp + fp)$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(38)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img514.png"
 ALT="$\displaystyle R$"></TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img313.png"
 ALT="$\textstyle =$"></TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="95" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img515.png"
 ALT="$\displaystyle tp/(tp + fn)$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(39)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
<A NAME="p:accuracy"></A> 
An obvious alternative that may occur to the reader is to 
judge an information retrieval system by its 
<A NAME="10654"></A> <I>accuracy</I> , that is, the fraction of its classifications that are
correct.  In 
terms of the contingency table above, <!-- MATH
 $\mbox{accuracy} = (tp + tn)/(tp +
fp + fn + tn)$
 -->
<IMG
 WIDTH="299" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img516.png"
 ALT="$\mbox{accuracy} = (tp + tn)/(tp +
fp + fn + tn)$">.  
This seems plausible, since there are two actual classes, relevant and
nonrelevant, and an information retrieval system can be thought of as a
two-class classifier which attempts to label them as such
(it retrieves the subset of documents which it believes to be relevant).
This is precisely the effectiveness measure often
used for evaluating machine learning classification problems.

<P>
There is a good reason why accuracy is not an appropriate measure for
information retrieval problems.  In almost all circumstances, the data
is extremely skewed: normally over 99.9% of the documents are in the 
nonrelevant category.  A system tuned to maximize
accuracy can appear to perform  well by simply deeming all documents nonrelevant 
to all queries. Even if
the system is quite good, trying
to label some documents as relevant will almost always lead to a
high rate of false positives.  However, 
labeling all documents as nonrelevant is
completely unsatisfying to an information retrieval system user.  Users are
always going to want to see some documents, and can be assumed to have a
certain tolerance for seeing some false positives providing that they
get some useful information.  The measures of precision and recall
concentrate the evaluation on the return of true positives, asking what
percentage of the relevant documents have been found and how many false
positives have also been returned.

<P>
The advantage of having the two numbers for precision and recall is that
one is more important than the other in many circumstances.  Typical web
surfers would like every result on the first page to be relevant (high
precision) but have not the slightest interest in knowing let alone
looking at every document that is relevant.  In contrast, various
professional searchers such as paralegals and intelligence analysts are
very concerned with trying to get as high recall as possible, and will
tolerate fairly low precision results in order to get it.  Individuals
searching their hard disks are also often interested in high recall
searches.  Nevertheless,
the two quantities clearly trade off against one another: you can always
get a recall of 1 (but very low precision) by retrieving all documents
for all queries!  Recall is a non-decreasing function of the number of
documents retrieved.  On the other hand, in a good system, precision
usually decreases as the number of documents retrieved is increased.
In general we want to get some amount of recall while tolerating only
a certain percentage of false positives.

<P>
A single measure that trades off precision versus
recall is the <A NAME="10657"></A> <I>F&nbsp;measure</I> , which is the weighted harmonic mean of
precision and recall:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
F = \frac{1}{\alpha\frac{1}{P} + (1-\alpha)\frac{1}{R}} 
= \frac{(\beta^2 + 1)PR}{\beta^2P + R} 
\mbox{\quad where \quad} \beta^2 = \frac{1-\alpha}{\alpha}
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="f-measure"></A><IMG
 WIDTH="400" HEIGHT="50" BORDER="0"
 SRC="img517.png"
 ALT="\begin{displaymath}
F = \frac{1}{\alpha\frac{1}{P} + (1-\alpha)\frac{1}{R}}
= \...
...
\mbox{\quad where \quad} \beta^2 = \frac{1-\alpha}{\alpha}
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(40)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
where <!-- MATH
 $\alpha \in [0,1]$
 -->
<IMG
 WIDTH="65" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img518.png"
 ALT="$\alpha \in [0,1]$"> and thus <!-- MATH
 $\beta^2
\in [0, \infty]$
 -->
<IMG
 WIDTH="79" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img519.png"
 ALT="$\beta^2
\in [0, \infty]$">.
The default 
<A NAME="10672"></A> <I>balanced F&nbsp;measure</I> 
equally weights precision and recall,
which means making <IMG
 WIDTH="61" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img520.png"
 ALT="$\alpha = 1/2$"> or <IMG
 WIDTH="44" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img521.png"
 ALT="$\beta = 1$">.
It is commonly written as <IMG
 WIDTH="19" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img522.png"
 ALT="$F_1$">, which is short for
<IMG
 WIDTH="36" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img523.png"
 ALT="$F_{\beta=1}$">, even though the formulation in terms of <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img524.png"
 ALT="$\alpha$">
more transparently exhibits the F&nbsp;measure as a weighted harmonic mean.
When using <IMG
 WIDTH="44" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img521.png"
 ALT="$\beta = 1$">, the formula on the right simplifies
to:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
F_{\beta=1} = \frac{2PR}{P+R}
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="98" HEIGHT="41" BORDER="0"
 SRC="img525.png"
 ALT="\begin{displaymath}
F_{\beta=1} = \frac{2PR}{P+R}
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(41)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
However, using an even weighting is not the only choice.  Values of
<IMG
 WIDTH="43" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img526.png"
 ALT="$\beta &lt; 1$"> emphasize precision, while values of <IMG
 WIDTH="44" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img527.png"
 ALT="$\beta &gt; 1$"> emphasize
recall.  For example, a value of <IMG
 WIDTH="43" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img528.png"
 ALT="$\beta=3$"> or <IMG
 WIDTH="44" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img529.png"
 ALT="$\beta=5$"> might be used if
recall is to be emphasized.
Recall, precision, and the F&nbsp;measure are
inherently measures between 0 and 1, but they are also very commonly
written as percentages, on a scale between 0 and 100.

<P>

<DIV ALIGN="CENTER">

<P><A NAME="fig:harmonic"></A><A NAME="p:harmonic"></A></P><IMG
 WIDTH="493" HEIGHT="345" ALIGN="BOTTOM" BORDER="0"
 SRC="img530.png"
 ALT="\includegraphics[totalheight=3in,clip=true]{HarmonicMean.eps}">
Graph comparing the harmonic mean to other means.The graph
  shows a slice through the calculation of various means of precision
  and recall for the fixed recall value of 70%. The
  harmonic mean is always less than either the arithmetic or geometric
  mean, and often quite close to the minimum of the two numbers.  When the
  precision is also 70%, all the measures coincide.
</DIV>

<P>
Why do we use a harmonic mean rather than the simpler
average (arithmetic mean)?  Recall that we can always get 100% recall by
just returning all documents, and therefore we can always get a 50%
arithmetic mean by the same process.  This strongly suggests that the
arithmetic mean is an unsuitable measure to use.
In contrast, if we assume that
1&nbsp;document in 10,000 is relevant to the query, the harmonic mean score of 
this strategy is 0.02%.  
The harmonic mean  is always less than or equal
to the arithmetic mean and the geometric mean.
When the values of two numbers differ greatly, the harmonic mean is
closer to their minimum than to 
their arithmetic mean; see Figure <A HREF="#fig:harmonic">8.1</A> .

<P>
<B>Exercises.</B>
<UL>
<LI>An IR system returns 8 relevant
documents, and 10 nonrelevant documents. There are a total
of 20 relevant documents in the collection. What is the
precision of the system on this search, and what is its
recall?

<P>
</LI>
<LI>The balanced F measure (a.k.a. F<IMG
 WIDTH="10" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.png"
 ALT="$_1$">)
is defined as the harmonic mean of precision and
recall. What is the advantage of using the harmonic mean
rather than ``averaging'' (using the arithmetic mean)?

<P>
</LI>
<LI>Derive the equivalence between the
two formulas for F measure shown in Equation&nbsp;<A HREF="#f-measure">40</A>, given
that <!-- MATH
 $\alpha = 1/(\beta^2 + 1)$
 -->
<IMG
 WIDTH="110" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img531.png"
 ALT="$\alpha = 1/(\beta^2 + 1)$">.

<P>
</LI>
</UL>

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html2431"
  HREF="evaluation-of-ranked-retrieval-results-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html2425"
  HREF="evaluation-in-information-retrieval-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html2419"
  HREF="standard-test-collections-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html2427"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html2429"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html2432"
  HREF="evaluation-of-ranked-retrieval-results-1.html">Evaluation of ranked retrieval</A>
<B> Up:</B> <A NAME="tex2html2426"
  HREF="evaluation-in-information-retrieval-1.html">Evaluation in information retrieval</A>
<B> Previous:</B> <A NAME="tex2html2420"
  HREF="standard-test-collections-1.html">Standard test collections</A>
 &nbsp; <B>  <A NAME="tex2html2428"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html2430"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
