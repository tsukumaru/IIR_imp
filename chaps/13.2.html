
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Naive Bayes text classification</TITLE>
<META NAME="description" CONTENT="Naive Bayes text classification">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="next" HREF="the-bernoulli-model-1.html">
<LINK REL="previous" HREF="the-text-classification-problem-1.html">
<LINK REL="up" HREF="text-classification-and-naive-bayes-1.html">
<LINK REL="next" HREF="relation-to-multinomial-unigram-language-model-1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html3461"
  HREF="relation-to-multinomial-unigram-language-model-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3455"
  HREF="text-classification-and-naive-bayes-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3449"
  HREF="the-text-classification-problem-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3457"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3459"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3462"
  HREF="relation-to-multinomial-unigram-language-model-1.html">Relation to multinomial unigram</A>
<B> Up:</B> <A NAME="tex2html3456"
  HREF="text-classification-and-naive-bayes-1.html">Text classification and Naive</A>
<B> Previous:</B> <A NAME="tex2html3450"
  HREF="the-text-classification-problem-1.html">The text classification problem</A>
 &nbsp; <B>  <A NAME="tex2html3458"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3460"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H1><A NAME="SECTION001820000000000000000"></A>
<A NAME="16262"></A><A NAME="sec:naivebayes"></A> <A NAME="p:naivebayes"></A>
<BR>
Naive Bayes text classification
</H1>  The first supervised learning method
we introduce is  the <A NAME="16265"></A> <I>multinomial Naive Bayes</I> 
or 
<A NAME="17761"></A> <I>multinomial NB</I> 
model, a probabilistic learning method. The probability of a document <IMG
 WIDTH="12" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img354.png"
 ALT="$d$"> being in
class <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$"> is computed as
<BR>
<DIV ALIGN="CENTER"><A NAME="eqn:multinomial3"></A>
<!-- MATH
 \begin{eqnarray}
P(c|d) \propto P(c) \prod_{1 \leq \tcposindex \leq n_d} P(\tcword_\tcposindex|c)
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="197" HEIGHT="52" ALIGN="MIDDLE" BORDER="0"
 SRC="img865.png"
 ALT="$\displaystyle P(c\vert d) \propto P(c) \prod_{1 \leq \tcposindex \leq n_d} P(\tcword_\tcposindex\vert c)$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(113)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
where <!-- MATH
 $P(\tcword_\tcposindex|c)$
 -->
<IMG
 WIDTH="52" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img866.png"
 ALT="$P(\tcword_\tcposindex\vert c)$"> is the conditional probability of
term <!-- MATH
 $\tcword_\tcposindex$
 -->
<IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img867.png"
 ALT="$\tcword_\tcposindex$"> occurring in a document of class
<IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$">.<A NAME="tex2html123"
  HREF="footnode.html#foot16273"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A>We interpret 
<!-- MATH
 $P(\tcword_\tcposindex|c)$
 -->
<IMG
 WIDTH="52" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img866.png"
 ALT="$P(\tcword_\tcposindex\vert c)$"> as a measure of how much evidence
<!-- MATH
 $\tcword_\tcposindex$
 -->
<IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img867.png"
 ALT="$\tcword_\tcposindex$"> contributes that <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$"> is the correct class.
<IMG
 WIDTH="35" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img870.png"
 ALT="$P(c)$"> is the prior probability of a document occurring in
class <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$">. If a document's terms do not provide clear
evidence for one class versus another, we choose the one that
has a higher prior probability.
<!-- MATH
 $\langle
\tcword_1,\tcword_2,\ldots,\tcword_{n_d}\rangle$
 -->
<IMG
 WIDTH="103" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img871.png"
 ALT="$\langle
\tcword_1,\tcword_2,\ldots,\tcword_{n_d}\rangle$"> are the
tokens in <IMG
 WIDTH="12" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img354.png"
 ALT="$d$"> that are part of the vocabulary we use for
classification and <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img872.png"
 ALT="$n_d$"> is the number of such tokens in <IMG
 WIDTH="12" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img354.png"
 ALT="$d$">. For example, <!-- MATH
 $\langle
\tcword_1,\tcword_2,\ldots,\tcword_{n_d}\rangle$
 -->
<IMG
 WIDTH="103" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img871.png"
 ALT="$\langle
\tcword_1,\tcword_2,\ldots,\tcword_{n_d}\rangle$"> for the
one-sentence document Beijing and Taipei join
the WTO might be <!-- MATH
 $\langle \term{Beijing}, \term{Taipei},
\term{join}, \term {WTO}\rangle$
 -->
<IMG
 WIDTH="171" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img873.png"
 ALT="$\langle \term{Beijing}, \term{Taipei},
\term{join}, \term {WTO}\rangle $">, with <IMG
 WIDTH="50" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img874.png"
 ALT="$n_d=4$">, if
we treat the terms and and the as stop words.

<P>
In text classification, our goal is to find the <I>best</I>
class for the document. The best class in NB classification
is the
most likely or 
<A NAME="16284"></A><A NAME="16285"></A> <I>maximum a posteriori</I> 
(<A NAME="16287"></A> <I>MAP</I> ) class <IMG
 WIDTH="34" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img875.png"
 ALT="$c_{map}$">:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
c_{map}  = 
\argmax_{\tcjclass \in \mathbb{C}} \hat{P}(\tcjclass|d) =
\argmax_{\tcjclass \in \mathbb{C}} \ \hat{P}(\tcjclass) \prod_{1 \leq \tcposindex \leq n_d}
\hat{P}(\tcword_\tcposindex|\tcjclass).
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="naivebayeseqword"></A><A NAME="eqn:multinomial5"></A><IMG
 WIDTH="435" HEIGHT="51" BORDER="0"
 SRC="img876.png"
 ALT="\begin{displaymath}c_{map} =
\argmax_{\tcjclass \in \mathbb{C}} \hat{P}(\tcjc...
...posindex \leq n_d}
\hat{P}(\tcword_\tcposindex\vert\tcjclass).
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(114)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
We write <IMG
 WIDTH="14" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img877.png"
 ALT="$\hat{P}$"> for <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$"> because we do not know the true
values of the parameters
<IMG
 WIDTH="35" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img878.png"
 ALT="$P(\tcjclass)$"> and
<!-- MATH
 $P(\tcword_\tcposindex|\tcjclass)$
 -->
<IMG
 WIDTH="52" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img879.png"
 ALT="$P(\tcword_\tcposindex\vert\tcjclass)$">, but estimate them from the
training set as we will see in a moment.

<P>
In Equation&nbsp;<A HREF="#eqn:multinomial5">114</A>,<A NAME="p:use-log-probabilities"></A> 
many conditional probabilities are
multiplied, one for each position <!-- MATH
 $1 \leq \tcposindex \leq n_d$
 -->
<IMG
 WIDTH="80" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img880.png"
 ALT="$1 \leq \tcposindex \leq n_d$">.
This can result in a floating point underflow. It
is therefore better to perform the computation by adding
logarithms of probabilities instead of multiplying
probabilities. The class with the highest log probability
score is still the most probable; <!-- MATH
 $\log (xy) = \log (x)
+ \log (y)$
 -->
<IMG
 WIDTH="187" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img881.png"
 ALT="$\log (xy) = \log (x)
+ \log (y)$"> and the logarithm function is monotonic. Hence,
the maximization that is actually done in most
implementations of NB is:
<BR>
<DIV ALIGN="CENTER"><A NAME="eqn:multinomial6"></A>
<!-- MATH
 \begin{eqnarray}
c_{map} = \argmax_{\tcjclass \in \mathbb{C}} \ [ \log \hat{P}(\tcjclass) +
\sum_{1 \leq \tcposindex \leq n_d}
\log \hat{P}(\tcword_\tcposindex|\tcjclass)].
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="325" HEIGHT="52" ALIGN="MIDDLE" BORDER="0"
 SRC="img882.png"
 ALT="$\displaystyle c_{map} = \argmax_{\tcjclass \in \mathbb{C}} \ [ \log \hat{P}(\tc...
...{1 \leq \tcposindex \leq n_d}
\log \hat{P}(\tcword_\tcposindex\vert\tcjclass)].$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(115)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
Equation&nbsp;<A HREF="#eqn:multinomial6">115</A> has a simple interpretation.  Each
conditional parameter <!-- MATH
 $\log
\hat{P}(\tcword_\tcposindex|\tcjclass)$
 -->
<IMG
 WIDTH="77" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img883.png"
 ALT="$\log
\hat{P}(\tcword_\tcposindex\vert\tcjclass)$"> is a weight that
indicates how good an indicator <!-- MATH
 $\tcword_\tcposindex$
 -->
<IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img867.png"
 ALT="$\tcword_\tcposindex$"> is for
<IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img884.png"
 ALT="$\tcjclass$">. Similarly, the prior <!-- MATH
 $\log \hat{P}(\tcjclass)$
 -->
<IMG
 WIDTH="60" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img885.png"
 ALT="$\log \hat{P}(\tcjclass)$">
is a weight that indicates the relative frequency of
<IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img884.png"
 ALT="$\tcjclass$">. More frequent classes are more likely to be the
correct class than infrequent
classes. 
The
sum of log prior and term weights is then a measure of how
much evidence there is for the document being in the class, and
Equation <A HREF="#eqn:multinomial6">115</A>  selects the class for which we have
the most evidence.

<P>
We will initially work with this intuitive interpretation of
the multinomial NB model and defer a formal derivation to
Section <A HREF="properties-of-naive-bayes-1.html#sec:generativemodel2">13.4</A> .

<P>
How do we estimate the parameters
<!-- MATH
 $\hat{P}(\tcjclass)$
 -->
<IMG
 WIDTH="35" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img886.png"
 ALT="$\hat{P}(\tcjclass)$"> and
<!-- MATH
 $\hat{P}(\tcword_\tcposindex|\tcjclass)$
 -->
<IMG
 WIDTH="52" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img887.png"
 ALT="$ \hat{P}(\tcword_\tcposindex\vert\tcjclass)$">?
We first try
the <A NAME="16319"></A> <A NAME="16320"></A> <I>maximum likelihood estimate</I>  (MLE; probtheory), which
is simply the relative frequency and
corresponds to the most likely value of each parameter given
the training data. For the priors this estimate is:
<BR>
<DIV ALIGN="CENTER"><A NAME="eqn:documentprior"></A>
<!-- MATH
 \begin{eqnarray}
\hat{P}(\tcjclass) = \frac{N_c}{N},
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="83" HEIGHT="53" ALIGN="MIDDLE" BORDER="0"
 SRC="img888.png"
 ALT="$\displaystyle \hat{P}(\tcjclass) = \frac{N_c}{N},$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(116)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
where <IMG
 WIDTH="23" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img889.png"
 ALT="$N_c$"><A NAME="Nj-notation"></A> is the number of documents in class <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img884.png"
 ALT="$\tcjclass$"> and
<IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img62.png"
 ALT="$N$"> is the total number of documents.

<P>
We estimate the conditional probability 
<!-- MATH
 $\hat{P}(\tcword|c)$
 -->
<IMG
 WIDTH="45" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img890.png"
 ALT="$\hat{P}(\tcword\vert c)$"> as the relative frequency
of term <IMG
 WIDTH="10" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img891.png"
 ALT="$\tcword$"> in
documents belonging to class <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$">:
<A NAME="p:tjknotation"></A> 
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\hat{P}(\tcword|c) = \frac{T_{c\tcword}}{\sum_{\tcword'
    \in V} T_{c\tcword'}},
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="eqn:condfirststab"></A><IMG
 WIDTH="136" HEIGHT="43" BORDER="0"
 SRC="img892.png"
 ALT="\begin{displaymath}
\hat{P}(\tcword\vert c) = \frac{T_{c\tcword}}{\sum_{\tcword'
\in V} T_{c\tcword'}},
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(117)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
where <IMG
 WIDTH="23" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img893.png"
 ALT="$T_{c\tcword}$"> is the number of occurrences of <IMG
 WIDTH="10" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img891.png"
 ALT="$\tcword$"> in
training documents from class <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$">,
including multiple
occurrences of a term in a document. We have made the
<A NAME="16340"></A> <I>positional independence assumption</I> here,
which we will discuss in more detail in the next section: 
<IMG
 WIDTH="23" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img893.png"
 ALT="$T_{c\tcword}$"> is a count of occurrences
in all positions <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$k$"> in the documents in the training set.
Thus, we do not compute different estimates for different
positions and, for example, if a word occurs twice in a document, in positions
<IMG
 WIDTH="18" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img774.png"
 ALT="$k_1$"> and <IMG
 WIDTH="18" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img894.png"
 ALT="$k_2$">, then 
<!-- MATH
 $\hat{P}(\tcword_{k_1}|c) 
= 
\hat{P}(\tcword_{k_2}|c)$
 -->
<IMG
 WIDTH="134" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img895.png"
 ALT="$
\hat{P}(\tcword_{k_1}\vert c)
=
\hat{P}(\tcword_{k_2}\vert c)
$">.

<P>
The problem with the MLE estimate is that it is zero for a
term-class combination that did not occur in the training
data. If
the term WTO in the training data only
occurred in China documents, then the MLE estimates
for the other classes, for example UK, will be
zero:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\hat{P}(\mbox{\term{WTO}}|\mbox{\class{UK}}) = 0.
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="118" HEIGHT="28" BORDER="0"
 SRC="img896.png"
 ALT="\begin{displaymath}
\hat{P}(\mbox{\term{WTO}}\vert\mbox{\class{UK}}) = 0.
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(118)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
Now, the one-sentence document Britain is a
member of the WTO
will get a conditional probability of
zero for UK because we are multiplying the conditional
probabilities for all terms in
Equation&nbsp;<A HREF="#eqn:multinomial3">113</A>. 
Clearly, the model should
assign a high probability to the UK class because
the term Britain
occurs. The problem is that the zero probability
for WTO cannot be ``conditioned away,'' no
matter how strong the evidence for the class UK
from other features. 
The estimate is 0 because of
<A NAME="16362"></A> <A NAME="p:sparseness"></A> <A NAME="16364"></A> <I>sparseness</I> : The training data are never large enough
to represent the frequency of rare events adequately, for
example, 
the frequency of WTO occurring in
UK documents.

<P>

<DIV ALIGN="CENTER"><A NAME="fig:multinomialalg"></A><A NAME="p:multinomialalg"></A><A NAME="16404"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 13.2:</STRONG>
Naive Bayes algorithm (multinomial model):
Training and testing.</CAPTION>
<TR><TD><IMG
 WIDTH="459" HEIGHT="386" BORDER="0"
 SRC="img897.png"
 ALT="\begin{figure}\begin{algorithm}{TrainMultinomialNB}{\mathbb{C},\docsetlabeled}
V...
...OR}\\
\RETURN{\argmax_{c \in \mathbb{C}} score[c]}
\end{algorithm}
\end{figure}"></TD></TR>
</TABLE>
</DIV>

<P>
To eliminate zeros, we use
<A NAME="16408"></A> <A NAME="16409"></A> <I>add-one</I> 
or <A NAME="16411"></A> <I>Laplace</I> 
<EM>smoothing</EM>, which simply
adds one to each count (cf.&nbsp;Section <A HREF="probability-estimates-in-theory-1.html#sec:probtheory">11.3.2</A> ):
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\hat{P}(\tcword|c) = \frac{T_{c\tcword}+1}{\sum_{\tcword' \in
  V}(T_{c\tcword'} + 1)}
= \frac{T_{c\tcword}+1}{(\sum_{\tcword' \in V} T_{c\tcword'})+B},
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="laplace"></A><IMG
 WIDTH="310" HEIGHT="44" BORDER="0"
 SRC="img898.png"
 ALT="\begin{displaymath}
\hat{P}(\tcword\vert c) = \frac{T_{c\tcword}+1}{\sum_{\tcwor...
...frac{T_{c\tcword}+1}{(\sum_{\tcword' \in V} T_{c\tcword'})+B},
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(119)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
where <IMG
 WIDTH="59" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img899.png"
 ALT="$B=\vert V\vert$"> is the number of terms in the vocabulary.
Add-one smoothing
can be interpreted as a uniform prior (each term occurs once
for each class) that is then updated as evidence
from the training data comes in. Note that this is
a prior probability for the occurrence of a <I>term</I> as opposed
to the prior probability of a <I>class</I> which we estimate in
Equation&nbsp;<A HREF="#eqn:documentprior">116</A> on the document level.

<P>
We have now introduced all the elements we need for training
and applying an NB classifier. The complete
algorithm is described in
Figure <A HREF="#fig:multinomialalg">13.2</A> .

<P>
<BR><P></P>
<DIV ALIGN="CENTER">

<A NAME="17780"></A>
<TABLE CELLPADDING=3 BORDER="1">
<CAPTION><STRONG>Table 13.1:</STRONG>
Data for parameter
estimation examples.  </CAPTION>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">docID</TD>
<TD ALIGN="LEFT">words in document</TD>
<TD ALIGN="LEFT">in <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$"> <IMG
 WIDTH="17" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img176.png"
 ALT="$=$"> China?</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">training set</TD>
<TD ALIGN="LEFT">1</TD>
<TD ALIGN="LEFT">Chinese Beijing Chinese</TD>
<TD ALIGN="LEFT">yes</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">2</TD>
<TD ALIGN="LEFT">Chinese Chinese Shanghai</TD>
<TD ALIGN="LEFT">yes</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">3</TD>
<TD ALIGN="LEFT">Chinese Macao</TD>
<TD ALIGN="LEFT">yes</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">4</TD>
<TD ALIGN="LEFT">Tokyo Japan Chinese</TD>
<TD ALIGN="LEFT">no</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">test set</TD>
<TD ALIGN="LEFT">5</TD>
<TD ALIGN="LEFT">Chinese Chinese Chinese Tokyo Japan</TD>
<TD ALIGN="LEFT">?</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
</TABLE>
</DIV>
<BR>

<P>
<B>Worked example.</B>
For the example in Table <A HREF="#tab:nbtoy">13.1</A> , the multinomial
parameters we need to classify the test document are the
priors <!-- MATH
 $\hat{P}(c) = 3/4$
 -->
<IMG
 WIDTH="83" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img900.png"
 ALT="$\hat{P}(c) = 3/4$"> and <!-- MATH
 $\hat{P}(\overline{c}) = 1/4$
 -->
<IMG
 WIDTH="83" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img901.png"
 ALT="$\hat{P}(\overline{c}) = 1/4$"> and the
following conditional probabilities:
<P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{eqnarray*}
\hat{P}(\term{Chinese}|c) &=& (5+1)/(8+6) = 6/14=3/7 \\
\hat{P}(\term{Tokyo}|c) = \hat{P}(\term{Japan}|c) &=& (0+1)/(8+6) = 1/14 \\
\hat{P}(\term{Chinese}|\overline{c}) &=& (1+1)/(3+6) = 2/9 \\
\hat{P}(\term{Tokyo}|\overline{c}) = 
\hat{P}(\term{Japan}|\overline{c}) &=& (1+1)/(3+6)= 2/9
\end{eqnarray*}
 -->
<IMG
 WIDTH="426" HEIGHT="100" BORDER="0"
 SRC="img902.png"
 ALT="\begin{eqnarray*}
\hat{P}(\term{Chinese}\vert c) &amp;=&amp; (5+1)/(8+6) = 6/14=3/7 \\
...
...) =
\hat{P}(\term{Japan}\vert\overline{c}) &amp;=&amp; (1+1)/(3+6)= 2/9
\end{eqnarray*}"></DIV>
<BR CLEAR="ALL"><P></P>
The denominators are <IMG
 WIDTH="53" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img903.png"
 ALT="$(8+6)$"> and <IMG
 WIDTH="53" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img904.png"
 ALT="$(3+6)$"> because 
the lengths of <IMG
 WIDTH="37" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img905.png"
 ALT="$text_c$"> and <!-- MATH
 $text_{\overline{c}}$
 -->
<IMG
 WIDTH="36" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img906.png"
 ALT="$text_{\overline{c}}$"> are 
8 and 3, respectively, and because 
the constant <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img168.png"
 ALT="$B$"> in
Equation&nbsp;<A HREF="#laplace">119</A> is 6 as the vocabulary consists of six
terms.

<P>
We then get:
<P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{eqnarray*}
\hat{P}(c|d_5) &\propto& 3/4 \cdot (3/7)^3 
\cdot 1/14 \cdot 1/14 \approx 0.0003 .\\
\hat{P}(\overline{c}|d_5) &\propto& 1/4 \cdot (2/9)^3
\cdot 2/9 \cdot 2/9 \approx 0.0001.
\end{eqnarray*}
 -->
<IMG
 WIDTH="337" HEIGHT="52" BORDER="0"
 SRC="img907.png"
 ALT="\begin{eqnarray*}
\hat{P}(c\vert d_5) &amp;\propto&amp; 3/4 \cdot (3/7)^3
\cdot 1/14 \c...
... &amp;\propto&amp; 1/4 \cdot (2/9)^3
\cdot 2/9 \cdot 2/9 \approx 0.0001.
\end{eqnarray*}"></DIV>
<BR CLEAR="ALL"><P></P>
Thus, the classifier assigns
the test document to <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$"> = China. The reason for
this classification decision is that
the three occurrences of the positive
indicator Chinese in <IMG
 WIDTH="19" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img908.png"
 ALT="$d_5$"> outweigh the occurrences of
the two negative indicators Japan and Tokyo. <B>End worked example.</B>

<P>
<BR><P></P>
<DIV ALIGN="CENTER">

<A NAME="17783"></A>
<TABLE CELLPADDING=3 BORDER="1">
<CAPTION><STRONG>Table 13.2:</STRONG>
Training and test times for
NB.  
</CAPTION>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">mode</TD>
<TD ALIGN="LEFT">time complexity</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">training</TD>
<TD ALIGN="LEFT"><!-- MATH
 $\Theta(|\docsetlabeled| L_{ave}+|\mathbb{C}||V|)$
 -->
<IMG
 WIDTH="147" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img909.png"
 ALT="$\Theta(\vert\docsetlabeled\vert L_{ave}+\vert\mathbb{C}\vert\vert V\vert)$"></TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">testing</TD>
<TD ALIGN="LEFT"><!-- MATH
 $\Theta( L_{a}+|\mathbb{C}| M_{a})= \Theta(|\mathbb{C}| M_{a})$
 -->
<IMG
 WIDTH="203" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img910.png"
 ALT="$\Theta( L_{a}+\vert\mathbb{C}\vert M_{a})= \Theta(\vert\mathbb{C}\vert M_{a})$"></TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
</TABLE>
</DIV>
<BR>
What is the time complexity of NB?  The complexity
of computing the parameters is <!-- MATH
 $\Theta(|\mathbb{C}||V|)$
 -->
<IMG
 WIDTH="75" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img911.png"
 ALT="$\Theta(\vert\mathbb{C}\vert\vert V\vert)$"> because
the set of parameters consists of <!-- MATH
 $|\mathbb{C}||V|$
 -->
<IMG
 WIDTH="48" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img912.png"
 ALT="$\vert\mathbb{C}\vert\vert V\vert$">
conditional probabilities and <IMG
 WIDTH="26" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img913.png"
 ALT="$\vert\mathbb{C}\vert$"> priors. The
preprocessing necessary for computing the parameters
(extracting the vocabulary, counting terms, etc.)  can be
done in one pass through the training data. The time
complexity of this component is therefore <!-- MATH
 $\Theta(|\docsetlabeled| L_{ave})$
 -->
<IMG
 WIDTH="83" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img914.png"
 ALT="$\Theta(\vert\docsetlabeled\vert L_{ave})$">,
where <!-- MATH
 $|\docsetlabeled|$
 -->
<IMG
 WIDTH="29" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img915.png"
 ALT="$\vert\docsetlabeled\vert$"> is the number of documents and <IMG
 WIDTH="32" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img185.png"
 ALT="$ L_{ave}$"> is
the average length of a document.  

<P>
<A NAME="p:dlenavetheta"></A> We use <!-- MATH
 $\Theta(|\docsetlabeled| L_{ave})$
 -->
<IMG
 WIDTH="83" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img914.png"
 ALT="$\Theta(\vert\docsetlabeled\vert L_{ave})$">
as a notation for <IMG
 WIDTH="41" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img124.png"
 ALT="$\Theta(T)$"> here, where <IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img123.png"
 ALT="$T$"> is the length of the
training collection. 
This is 
nonstandard;
<IMG
 WIDTH="34" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img916.png"
 ALT="$\Theta(.)$"> is not defined for an average. 
We prefer expressing the time
complexity in terms of <!-- MATH
 $\docsetlabeled$
 -->
<IMG
 WIDTH="18" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img856.png"
 ALT="$\docsetlabeled$"> and <IMG
 WIDTH="32" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img185.png"
 ALT="$ L_{ave}$"> 
because these are the primary statistics used to
characterize training collections.

<P>
The time complexity of
A<SMALL>PPLY</SMALL>M<SMALL>ULTINOMIAL</SMALL>NB in
Figure <A HREF="#fig:multinomialalg">13.2</A>  is
<!-- MATH
 $\Theta(|\mathbb{C}| L_{a})$
 -->
<IMG
 WIDTH="69" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img917.png"
 ALT="$\Theta(\vert\mathbb{C}\vert L_{a})$">.
<IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img918.png"
 ALT="$ L_{a}$"> and <IMG
 WIDTH="27" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img919.png"
 ALT="$ M_{a}$"> are the numbers of
tokens and types, respectively, in the test
document<A NAME="p:dlentest-notation"></A> .
A<SMALL>PPLY</SMALL>M<SMALL>ULTINOMIAL</SMALL>NB can be modified to be
<!-- MATH
 $\Theta( L_{a}+|\mathbb{C}| M_{a})$
 -->
<IMG
 WIDTH="110" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img920.png"
 ALT="$\Theta( L_{a}+\vert\mathbb{C}\vert M_{a})$">
(Exercise <A HREF="evaluation-of-text-classification-1.html#ex:multinomialtimecomplexity">13.6</A> ).
Finally, assuming
that the length of test documents is bounded,
<A NAME="p:dlentestdvoctestsimplification"></A> <!-- MATH
 $\Theta( L_{a}+|\mathbb{C}| M_{a})=
\Theta(|\mathbb{C}| M_{a})$
 -->
<IMG
 WIDTH="203" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img910.png"
 ALT="$\Theta( L_{a}+\vert\mathbb{C}\vert M_{a})= \Theta(\vert\mathbb{C}\vert M_{a})$"> because 
<!-- MATH
 $L_{a} < b |C| M_{a}$
 -->
<IMG
 WIDTH="94" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img921.png"
 ALT="$ L_{a} &lt; b \vert C\vert M_{a}$"> for a fixed constant <IMG
 WIDTH="12" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img137.png"
 ALT="$b$">.<A NAME="tex2html127"
  HREF="footnode.html#foot17785"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A>
<P>
Table <A HREF="#tab:nbtimecomplexity">13.2</A>  summarizes the time complexities.
In general, we have <!-- MATH
 $|\mathbb{C}||V| < |\docsetlabeled| L_{ave}$
 -->
<IMG
 WIDTH="123" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img923.png"
 ALT="$\vert\mathbb{C}\vert\vert V\vert &lt; \vert\docsetlabeled\vert L_{ave}$">, so both training
and testing complexity are linear in the time it takes
to scan the data. Because we have to look at the data at
least once, NB can be said to have optimal time
complexity. Its efficiency is one reason why NB
is a popular text classification method.

<P>
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><A NAME="tex2html3463"
  HREF="relation-to-multinomial-unigram-language-model-1.html">Relation to multinomial unigram language model</A>
</UL>
<!--End of Table of Child-Links-->
<HR>
<!--Navigation Panel-->
<A NAME="tex2html3461"
  HREF="relation-to-multinomial-unigram-language-model-1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html3455"
  HREF="text-classification-and-naive-bayes-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html3449"
  HREF="the-text-classification-problem-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html3457"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html3459"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html3462"
  HREF="relation-to-multinomial-unigram-language-model-1.html">Relation to multinomial unigram</A>
<B> Up:</B> <A NAME="tex2html3456"
  HREF="text-classification-and-naive-bayes-1.html">Text classification and Naive</A>
<B> Previous:</B> <A NAME="tex2html3450"
  HREF="the-text-classification-problem-1.html">The text classification problem</A>
 &nbsp; <B>  <A NAME="tex2html3458"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html3460"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
