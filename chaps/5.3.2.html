
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Gamma codes</TITLE>
<META NAME="description" CONTENT="Gamma codes">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook.css">

<LINK REL="previous" HREF="variable-byte-codes-1.html">
<LINK REL="up" HREF="postings-file-compression-1.html">
<LINK REL="next" HREF="references-and-further-reading-5.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html1791"
  HREF="references-and-further-reading-5.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html1785"
  HREF="postings-file-compression-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html1781"
  HREF="variable-byte-codes-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html1787"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html1789"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html1792"
  HREF="references-and-further-reading-5.html">References and further reading</A>
<B> Up:</B> <A NAME="tex2html1786"
  HREF="postings-file-compression-1.html">Postings file compression</A>
<B> Previous:</B> <A NAME="tex2html1782"
  HREF="variable-byte-codes-1.html">Variable byte codes</A>
 &nbsp; <B>  <A NAME="tex2html1788"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html1790"
  HREF="index-1.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION001032000000000000000"></A><A NAME="sec:gamma-code"></A> <A NAME="p:gamma-code"></A>
<BR>
Gamma codes
</H2> 

<P>
<BR><P></P>
<DIV ALIGN="CENTER">

<A NAME="6427"></A>
<TABLE CELLPADDING=3 BORDER="1">
<CAPTION><STRONG>Table 5.5:</STRONG>
Some examples of unary and <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes.
Unary codes are only shown for the smaller numbers.
Commas in <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes are for readability only and are not part of the actual codes.</CAPTION>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">number</TD>
<TD ALIGN="LEFT">unary code</TD>
<TD ALIGN="LEFT">length</TD>
<TD ALIGN="LEFT">offset</TD>
<TD ALIGN="LEFT"><IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">0</TD>
<TD ALIGN="LEFT">0</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">1</TD>
<TD ALIGN="LEFT">10</TD>
<TD ALIGN="LEFT">0</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">0</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">2</TD>
<TD ALIGN="LEFT">110</TD>
<TD ALIGN="LEFT">10</TD>
<TD ALIGN="LEFT">0</TD>
<TD ALIGN="LEFT">10,0</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">3</TD>
<TD ALIGN="LEFT">1110</TD>
<TD ALIGN="LEFT">10</TD>
<TD ALIGN="LEFT">1</TD>
<TD ALIGN="LEFT">10,1</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">4</TD>
<TD ALIGN="LEFT">11110</TD>
<TD ALIGN="LEFT">110</TD>
<TD ALIGN="LEFT">00</TD>
<TD ALIGN="LEFT">110,00</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">9</TD>
<TD ALIGN="LEFT">1111111110</TD>
<TD ALIGN="LEFT">1110</TD>
<TD ALIGN="LEFT">001</TD>
<TD ALIGN="LEFT">1110,001</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">13</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">1110</TD>
<TD ALIGN="LEFT">101</TD>
<TD ALIGN="LEFT">1110,101</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">24</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">11110</TD>
<TD ALIGN="LEFT">1000</TD>
<TD ALIGN="LEFT">11110,1000</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">511</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">111111110</TD>
<TD ALIGN="LEFT">11111111</TD>
<TD ALIGN="LEFT">111111110,11111111</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">1025</TD>
<TD ALIGN="LEFT">&nbsp;</TD>
<TD ALIGN="LEFT">11111111110</TD>
<TD ALIGN="LEFT">0000000001</TD>
<TD ALIGN="LEFT">11111111110,0000000001</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
</TABLE>
<A NAME="tab:icompresstb3"></A> <A NAME="p:icompresstb3"></A>  

</DIV>
<BR>

<P>
VB codes use an adaptive number of <I>bytes</I>
depending on the size of the gap. Bit-level codes adapt the
length of the code on the finer grained <I>bit</I> level.
The simplest bit-level code is <A NAME="6437"></A> <I>unary code</I> . The unary
code of <IMG
 WIDTH="13" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img104.png"
 ALT="$n$"> is a string of <IMG
 WIDTH="13" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img104.png"
 ALT="$n$"> 1s followed by a 0 (see the
first two columns of Table <A HREF="#tab:icompresstb3">5.5</A> ).
Obviously,
this is not a very efficient code, but it will come in handy
in a moment.

<P>
How efficient can a code be in principle?  Assuming the
<IMG
 WIDTH="20" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img283.png"
 ALT="$2^n$"> gaps <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img284.png"
 ALT="$G$"> with <!-- MATH
 $1 \leq G \leq 2^{n}$
 -->
<IMG
 WIDTH="84" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img285.png"
 ALT="$1 \leq G \leq 2^{n}$"> are all equally
likely, the optimal encoding uses <IMG
 WIDTH="13" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img104.png"
 ALT="$n$"> bits for each <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img284.png"
 ALT="$G$">.  So
some gaps (<IMG
 WIDTH="54" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img286.png"
 ALT="$G=2^{n}$"> in this case) cannot be encoded with
fewer than <IMG
 WIDTH="48" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img287.png"
 ALT="$\log_2 G$"> bits.  Our goal is to get as close to
this lower bound as possible.

<P>
A method that is within a factor of optimal
is <A NAME="6442"></A> <I><IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoding</I> .
<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $">
codes implement variable-length encoding by splitting the
representation of a
gap <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img284.png"
 ALT="$G$"> into a pair of <I>length</I> and <I>offset</I>.
<I>Offset</I> is <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img284.png"
 ALT="$G$"> in binary, but with the leading 1
removed.<A NAME="tex2html56"
  HREF="footnode.html#foot6447"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A> For example, for 13 (binary 1101) <I>offset</I> is 101. 
<I>Length</I> encodes the length of <I>offset</I>
in unary code. 
For 13,
the length of <I>offset</I> is 3 bits, which is 1110 in
unary. The <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code of 13 is therefore 1110101, the
concatenation of length 1110 and offset 101.
The right hand column of Table <A HREF="#tab:icompresstb3">5.5</A>  gives additional
examples of <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes. 

<P>
A <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code is decoded by first reading the unary code
up to the 0 that terminates it, for example, the four bits 1110
when decoding 1110101. Now we know how long the
offset is: 3 bits. The offset 101 can then be read correctly and
the 1 that was chopped off in encoding is prepended: 101
<IMG
 WIDTH="21" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img97.png"
 ALT="$\rightarrow$"> 1101 = 13.

<P>
The length of <I>offset</I> is
<!-- MATH
 $\lfloor \log_2 G
\rfloor$
 -->
<IMG
 WIDTH="63" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img288.png"
 ALT="$\lfloor \log_2 G
\rfloor$"> bits and the length of <I>length</I> 
is <!-- MATH
 $\lfloor \log_2 G
\rfloor +1$
 -->
<IMG
 WIDTH="91" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img289.png"
 ALT="$\lfloor \log_2 G
\rfloor +1$"> bits, 
so the length of the entire
code is
<!-- MATH
 $2 \times \lfloor \log_2 G \rfloor +1$
 -->
<IMG
 WIDTH="118" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img290.png"
 ALT="$2 \times \lfloor \log_2 G \rfloor +1$"> bits. <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes are
always of odd length and they are within a factor of
2 of what we claimed to be 
the optimal encoding length <IMG
 WIDTH="48" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img287.png"
 ALT="$\log_2 G$">.
We derived this optimum
from the assumption
that the <IMG
 WIDTH="20" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img283.png"
 ALT="$2^n$"> gaps
between <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img291.png"
 ALT="$1$"> and <IMG
 WIDTH="19" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img292.png"
 ALT="$2^{n}$"> are equiprobable.
But this need not be the case. In general, we do not know the
probability distribution over gaps a priori.

<P>

<DIV ALIGN="CENTER"><A NAME="fig:entropydef"></A><A NAME="p:entropydef"></A><A NAME="6458"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 5.9:</STRONG>

Entropy <IMG
 WIDTH="42" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.png"
 ALT="$H(P)$"> as a function of <IMG
 WIDTH="43" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img14.png"
 ALT="$P(x_1)$"> for a sample space
with two outcomes <IMG
 WIDTH="19" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img15.png"
 ALT="$x_1$"> and <IMG
 WIDTH="19" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img16.png"
 ALT="$x_2$">.
</CAPTION>
<TR><TD><IMG
 WIDTH="313" HEIGHT="285" ALIGN="BOTTOM" BORDER="0"
 SRC="img293.png"
 ALT="\includegraphics[width=7cm]{art/entropy.eps}"></TD></TR>
</TABLE>
</DIV>
The characteristic of a discrete probability
distribution<A NAME="tex2html58"
  HREF="footnode.html#foot6755"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A> <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$"> that
determines its coding properties (including whether a code is
optimal) is its
<A NAME="6464"></A> <A NAME="p:entropy"></A> <A NAME="6466"></A> <I>entropy</I>  <IMG
 WIDTH="42" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.png"
 ALT="$H(P)$">, which is
defined as follows:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
H(P) = - \sum_{x \in X} P(x) \log_2 P(x)
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="201" HEIGHT="43" BORDER="0"
 SRC="img294.png"
 ALT="\begin{displaymath}
H(P) = - \sum_{x \in X} P(x) \log_2 P(x)
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(4)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
where <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img295.png"
 ALT="$X$"> is the set of all possible numbers  we need to be
able to encode
(and therefore <!-- MATH
 $\sum_{x \in X} P(x) = 1.0$
 -->
<IMG
 WIDTH="119" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img296.png"
 ALT="$\sum_{x \in X} P(x) = 1.0$">). Entropy is a
measure of uncertainty as shown in Figure <A HREF="#fig:entropydef">5.9</A>  for a
probability distribution <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$"> over two possible outcomes, namely,
<!-- MATH
 $X = \{ x_1 , x_2 \}$
 -->
<IMG
 WIDTH="94" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img297.png"
 ALT="$X = \{ x_1 , x_2 \} $">. Entropy is maximized (<IMG
 WIDTH="72" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img298.png"
 ALT="$H(P)=1$">) for
<!-- MATH
 $P(x_1) = P(x_2) = 0.5$
 -->
<IMG
 WIDTH="147" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img299.png"
 ALT="$P(x_1) = P(x_2) = 0.5$"> when uncertainty about which <IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img300.png"
 ALT="$x_i$">
will appear next is largest; and minimized (<IMG
 WIDTH="72" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img301.png"
 ALT="$H(P)=0$">) for 
<!-- MATH
 $P(x_1) =1, P(x_2)=0$
 -->
<IMG
 WIDTH="150" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img302.png"
 ALT="$P(x_1) =1, P(x_2)=0$">
and for <!-- MATH
 $P(x_1) =0, P(x_2)=1$
 -->
<IMG
 WIDTH="150" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img303.png"
 ALT="$P(x_1) =0, P(x_2)=1$">
when there is absolute certainty.

<P>
It can be shown
that the lower bound for the expected length <IMG
 WIDTH="37" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img304.png"
 ALT="$E(L)$"> of a
code <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img127.png"
 ALT="$L$"> is <IMG
 WIDTH="42" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.png"
 ALT="$H(P)$"> if certain conditions hold (see the references). It can
further be shown that for <!-- MATH
 $1 < H(P) < \infty$
 -->
<IMG
 WIDTH="108" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img305.png"
 ALT="$1 &lt; H(P) &lt; \infty$">, <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $">
encoding is within a factor of 3 of this optimal encoding,
approaching 2 for large <IMG
 WIDTH="42" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.png"
 ALT="$H(P)$">:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\frac{E(L_{\gamma})}{H(P)} \leq 2+\frac{1}{H(P)} \leq 3.
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="169" HEIGHT="45" BORDER="0"
 SRC="img306.png"
 ALT="\begin{displaymath}
\frac{E(L_{\gamma})}{H(P)} \leq 2+\frac{1}{H(P)} \leq 3.
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(5)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
What is remarkable about this result is that it holds for
any probability distribution <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$">. So without knowing
anything about the properties of the distribution of gaps,
we can apply <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes and be certain that they are within a
factor of <IMG
 WIDTH="27" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img307.png"
 ALT="$\approx \!2$"> of the optimal code for distributions
of large entropy. A code like <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code with the property of
being within a factor of optimal for an arbitrary distribution <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$"> is
called 
<A NAME="6479"></A> <I>universal</I> . 

<P>
In addition to universality,
<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes have two other properties that are useful for index
compression. First, they are 
<A NAME="6481"></A> <I>prefix free</I> , namely, no
<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code is the prefix of another. This means that
there is always a unique decoding of a sequence of
<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes - and we do not need delimiters between them,
which would decrease the efficiency of the code. The second
property is that <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes are
<A NAME="6483"></A> <I>parameter free</I> . For many other efficient codes, we
have to fit the parameters of a model (e.g., the 
binomial distribution) to
the distribution of gaps in the index. This complicates the
implementation of compression and decompression. 
For instance, the
parameters need to be stored and retrieved. And in dynamic indexing, the distribution of
gaps can change, so that the original parameters are no longer
appropriate. These problems are avoided with a
parameter-free code.

<P>
How much compression of the inverted index do <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes
achieve? To answer this question we use Zipf's law, the term
distribution model introduced in Section <A HREF="zipfs-law-modeling-the-distribution-of-terms-1.html#sec:zipf">5.1.2</A> .
According to Zipf's law, the collection frequency <IMG
 WIDTH="21" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img245.png"
 ALT="$\collf_i$"> is proportional to the
inverse of the rank <IMG
 WIDTH="8" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.png"
 ALT="$i$">, that is, there is a constant <IMG
 WIDTH="15" HEIGHT="35" ALIGN="MIDDLE" BORDER="0"
 SRC="img308.png"
 ALT="$c'$"> such that:
<BR>
<DIV ALIGN="CENTER">

<!-- MATH
 \begin{eqnarray}
\collf_i = \frac{c'}{i}.
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="62" HEIGHT="55" ALIGN="MIDDLE" BORDER="0"
 SRC="img309.png"
 ALT="$\displaystyle \collf_i = \frac{c'}{i}.$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(6)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
We can choose a different constant <IMG
 WIDTH="11" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img252.png"
 ALT="$c$"> such that the 
fractions <IMG
 WIDTH="25" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img310.png"
 ALT="$c/i$">
are relative
frequencies and sum to 1 (that is, 
<!-- MATH
 $c/i =
\collf_i / T$
 -->
<IMG
 WIDTH="85" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img311.png"
 ALT="$c/i =
\collf_i / T$">):
<BR>
<DIV ALIGN="CENTER">

<!-- MATH
 \begin{eqnarray}
1 = \sum_{i=1}^{M} \frac{c}{i} = c \sum_{i=1}^{M}
\frac{1}{i} & = & c \ H_M \\
c   =  \frac{1}{H_M}
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="133" HEIGHT="64" ALIGN="MIDDLE" BORDER="0"
 SRC="img312.png"
 ALT="$\displaystyle 1 = \sum_{i=1}^{M} \frac{c}{i} = c \sum_{i=1}^{M}
\frac{1}{i}$"></TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img313.png"
 ALT="$\textstyle =$"></TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="41" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img314.png"
 ALT="$\displaystyle c \ H_M$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(7)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="63" HEIGHT="52" ALIGN="MIDDLE" BORDER="0"
 SRC="img315.png"
 ALT="$\displaystyle c = \frac{1}{H_M}$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=10 ALIGN="RIGHT">
(8)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
where <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img186.png"
 ALT="$M$"> is the number of distinct terms and <IMG
 WIDTH="30" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img316.png"
 ALT="$H_M$">
is the
<IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img186.png"
 ALT="$M$">th <A NAME="6502"></A> <A NAME="p:harmonicnumber"></A> <A NAME="6504"></A> <I>harmonic
  number</I> .
<A NAME="tex2html59"
  HREF="footnode.html#foot6506"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/footnote.png"></SUP></A> Reuters-RCV1 has
<IMG
 WIDTH="94" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img318.png"
 ALT="$M= 400{,}000$"> distinct terms
and  <!-- MATH
 $H_M\approx
\ln M$
 -->
<IMG
 WIDTH="85" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img319.png"
 ALT="$H_M\approx
\ln M$">, so we have 
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
c = \frac{1}{H_M} \approx \frac{1}{\ln M} = \frac{1}{\ln
400{,}000} \approx \frac{1}{13}.
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="257" HEIGHT="42" BORDER="0"
 SRC="img320.png"
 ALT="\begin{displaymath}
c = \frac{1}{H_M} \approx \frac{1}{\ln M} = \frac{1}{\ln
400{,}000} \approx \frac{1}{13}.
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(9)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
Thus the <IMG
 WIDTH="8" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.png"
 ALT="$i$">th term has a relative frequency of roughly
<IMG
 WIDTH="56" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img321.png"
 ALT="$1/(13i)$">, and
the expected average number of occurrences of term
<IMG
 WIDTH="8" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.png"
 ALT="$i$"> in a document of length <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img127.png"
 ALT="$L$"> is:
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
L   \frac{c}{i}  \approx
\frac{200 \times \frac{1}{13}}{i} \approx \frac{15}{i}
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="145" HEIGHT="44" BORDER="0"
 SRC="img322.png"
 ALT="\begin{displaymath}
L \frac{c}{i} \approx
\frac{200 \times \frac{1}{13}}{i} \approx \frac{15}{i}
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(10)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
where we interpret the relative frequency as a term occurrence
probability. Recall that 200 is the average number of
tokens per document in Reuters-RCV1 (Table <A HREF="blocked-sort-based-indexing-1.html#tab:icompresstb1">4.2</A> ).

<P>

<DIV ALIGN="CENTER"><A NAME="icompressfg3"></A><A NAME="6760"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 5.10:</STRONG>
Stratification of terms for
estimating the size of a <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoded inverted index.</CAPTION>
<TR><TD><IMG
 WIDTH="264" HEIGHT="227" BORDER="0"
 SRC="img323.png"
 ALT="\begin{figure}\begin{tabular}{l\vert c@{}\vert}
\par
&amp;\multicolumn{1}{\vert c\ve...
...}\\ \cline{2-2}
terms&amp;\\ \hline\hline
\ldots &amp; \ldots
\end{tabular}
\end{figure}"></TD></TR>
</TABLE>
</DIV>
Now we have derived term
statistics that characterize the distribution of terms in
the collection and, by extension, the distribution of gaps in
the postings lists.
From these statistics, we can
calculate
the space requirements for an inverted index compressed with
<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoding. We first stratify the 
vocabulary into blocks of size
<IMG
 WIDTH="58" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img324.png"
 ALT="$ L c=15$">. 
On average, term <IMG
 WIDTH="8" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.png"
 ALT="$i$"> occurs <IMG
 WIDTH="35" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img325.png"
 ALT="$15/i$"> times per
document. So the average number of occurrences
<IMG
 WIDTH="13" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img326.png"
 ALT="$\overline{f}$"> per document
is <!-- MATH
 $1 \leq \overline{f}$
 -->
<IMG
 WIDTH="43" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img327.png"
 ALT="$1 \leq \overline{f} $"> for terms in the
first block, corresponding to a total number of <IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img62.png"
 ALT="$N$"> gaps per
term. The average is
<!-- MATH
 $\frac{1}{2} \leq \overline{f} < 1$
 -->
<IMG
 WIDTH="75" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img328.png"
 ALT="$\frac{1}{2} \leq \overline{f} &lt; 1$"> for terms in the
second block, corresponding to <IMG
 WIDTH="36" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img329.png"
 ALT="$N/2$"> gaps per term, and
<!-- MATH
 $\frac{1}{3} \leq \overline{f} < \frac{1}{2}$
 -->
<IMG
 WIDTH="77" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img330.png"
 ALT="$\frac{1}{3} \leq \overline{f} &lt; \frac{1}{2}$"> for terms in the
third block, corresponding to <IMG
 WIDTH="36" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img331.png"
 ALT="$N/3$"> gaps per term, and so on. (We
take the lower bound because it simplifies subsequent calculations.
As we will see, the final estimate is too
pessimistic, even with this assumption.)
We will make the somewhat unrealistic assumption that all
gaps for a given term have the same size
as shown in Figure&nbsp;<A HREF="#icompressfg3">5.10</A>.
Assuming such a uniform distribution of gaps,
we then have gaps of size 1 in block 1, gaps of size 2 in
block 2, and so on.

<P>
Encoding the <IMG
 WIDTH="33" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img332.png"
 ALT="$N/j$"> gaps of size <IMG
 WIDTH="9" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$j$"> with <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes, the number of
bits needed for the postings list
of a term in the <IMG
 WIDTH="9" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.png"
 ALT="$j$">th block (corresponding to one row in
the figure) is:
<P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{eqnarray*}
\mbox{\em bits-per-row}&=&\frac{N}{j}  \times 
(2 \times \lfloor \log_2 j \rfloor +1)\\
&\approx& 
\frac{2N  \log_2 j}{j}.
\end{eqnarray*}
 -->
<IMG
 WIDTH="281" HEIGHT="88" BORDER="0"
 SRC="img333.png"
 ALT="\begin{eqnarray*}
\mbox{\em bits-per-row}&amp;=&amp;\frac{N}{j} \times
(2 \times \lfloor \log_2 j \rfloor +1)\\
&amp;\approx&amp;
\frac{2N \log_2 j}{j}.
\end{eqnarray*}"></DIV>
<BR CLEAR="ALL"><P></P>
To encode the entire block, we need 
<!-- MATH
 $( L c) \cdot (2N\log_2 j)/j$
 -->
<IMG
 WIDTH="136" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img334.png"
 ALT="$( L c) \cdot (2N\log_2 j)/j$"> bits. There are <IMG
 WIDTH="61" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img335.png"
 ALT="$M/( L c)$">
blocks, so the postings file as a whole will
take up:
<BR>
<DIV ALIGN="CENTER"><A NAME="icompresseq1"></A>
<!-- MATH
 \begin{eqnarray}
&&\sum_{j=1}^{\frac{M}{ L c}}  \frac{2N   L c
\log_2 j}{j} .
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD>&nbsp;</TD>
<TD ALIGN="LEFT" NOWRAP><IMG
 WIDTH="112" HEIGHT="76" ALIGN="MIDDLE" BORDER="0"
 SRC="img336.png"
 ALT="$\displaystyle \sum_{j=1}^{\frac{M}{ L c}} \frac{2N L c
\log_2 j}{j} .$"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(11)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>

<P>
For Reuters-RCV1, <!-- MATH
 $\frac{M}{ L c} \approx$
 -->
<IMG
 WIDTH="38" HEIGHT="39" ALIGN="MIDDLE" BORDER="0"
 SRC="img337.png"
 ALT="$\frac{M}{ L c} \approx$"> 400,000<!-- MATH
 $/15
\approx 27{,}000$
 -->
<IMG
 WIDTH="95" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img338.png"
 ALT="$/15
\approx 27{,}000$"> and 
<BR>
<DIV ALIGN="RIGHT">

<!-- MATH
 \begin{equation}
\sum_{j=1}^{27{,}000}  \frac{2 \times 10^6 \times 15 \log_2 j}{j} \approx
224 \ \mbox{MB}.
\end{equation}
 -->
<TABLE WIDTH="100%" ALIGN="CENTER">
<TR VALIGN="MIDDLE"><TD ALIGN="CENTER" NOWRAP><A NAME="totalindexsizeeq"></A><IMG
 WIDTH="244" HEIGHT="56" BORDER="0"
 SRC="img339.png"
 ALT="\begin{displaymath}
\sum_{j=1}^{27{,}000} \frac{2 \times 10^6 \times 15 \log_2 j}{j} \approx
224 \ \mbox{MB}.
\end{displaymath}"></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(12)</TD></TR>
</TABLE>
<BR CLEAR="ALL"></DIV><P></P>
So the postings file of the compressed inverted index 
for our 960 MB collection has a size of 224 MB, one fourth the
size of
the original collection.

<P>
When we run <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> compression on Reuters-RCV1,
the actual size of the compressed index is even
lower: 101 MB, a bit more than one tenth of the size of the
collection. The reason for the discrepancy between
predicted and actual value is
that (i) Zipf's law is not a very good
approximation of the actual distribution of term frequencies
for Reuters-RCV1 and (ii) gaps are not uniform. The Zipf model predicts an index size of 251 MB
for the unrounded numbers from Table <A HREF="blocked-sort-based-indexing-1.html#tab:icompresstb1">4.2</A> . If
term frequencies are generated from the Zipf model and a
compressed index is created for these artificial terms, then
the compressed size is 254 MB. So to the extent that
the assumptions about the distribution of term frequencies
are accurate,
the predictions of the model are correct.

<P>
<BR><P></P>
<DIV ALIGN="CENTER">

<A NAME="6763"></A>
<TABLE CELLPADDING=3 BORDER="1">
<CAPTION><STRONG>Table:</STRONG>
Index and dictionary compression for Reuters-RCV1.
The compression ratio depends on the proportion of actual text
in the collection. Reuters-RCV1 contains a
large amount of XML
markup. Using the two best
compression schemes, <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoding and blocking with
front coding, the 
ratio compressed index to collection size is therefore
especially small for Reuters-RCV1:
<!-- MATH
 $(101+7.9)/3600 \approx 0.03$
 -->
<IMG
 WIDTH="172" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img340.png"
 ALT="$(101+7.9)/3600 \approx 0.03$">.<!-- MATH
 $(101+5.9)/3600 \approx 0.03$
 -->
<IMG
 WIDTH="173" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img17.png"
 ALT="$(101+5.9)/3600 \approx 0.03$">.
</CAPTION>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">data structure</TD>
<TD ALIGN="RIGHT">size in MB</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">dictionary, fixed-width</TD>
<TD ALIGN="RIGHT">19.211.2</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">dictionary, term pointers into string</TD>
<TD ALIGN="RIGHT">10.8 7.6</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT"><IMG
 WIDTH="17" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img275.png"
 ALT="$\sim$">, with blocking, <IMG
 WIDTH="42" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img11.png"
 ALT="$k=4$"></TD>
<TD ALIGN="RIGHT">10.3 7.1</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT"><IMG
 WIDTH="17" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img275.png"
 ALT="$\sim$">, with blocking &amp; front coding</TD>
<TD ALIGN="RIGHT">7.9 5.9</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">collection (text, xml markup etc)</TD>
<TD ALIGN="RIGHT">3600.0</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">collection (text)</TD>
<TD ALIGN="RIGHT">960.0</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">term incidence matrix</TD>
<TD ALIGN="RIGHT">40,000.0</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">postings, uncompressed (32-bit words)</TD>
<TD ALIGN="RIGHT">400.0</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">postings, uncompressed (20 bits)</TD>
<TD ALIGN="RIGHT">250.0</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">postings, variable byte encoded</TD>
<TD ALIGN="RIGHT">116.0</TD>
</TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT">postings, <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoded</TD>
<TD ALIGN="RIGHT">101.0</TD>
</TR>
</TABLE>
<A NAME="tab:summaryicompress"></A> <A NAME="p:summaryicompress"></A>  

</DIV>
<BR>

<P>
Table <A HREF="#tab:summaryicompress">5.6</A>  summarizes
the compression techniques covered in this chapter.
The
term incidence matrix 
(Figure <A HREF="an-example-information-retrieval-problem-1.html#fig:termdoc">1.1</A> , page <A HREF="an-example-information-retrieval-problem-1.html#p:termdoc">1.1</A> )
for Reuters-RCV1 has size <!-- MATH
 $400{,}000 \times
800{,}000 = 40 \times 8 \times 10^9$
 -->
<IMG
 WIDTH="235" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img341.png"
 ALT="$400{,}000 \times
800{,}000 = 40 \times 8 \times 10^9 $"> bits or 40 GB.
The numbers were the collection (3600 MB and 960 MB) are for
the encoding of RCV1 of CD, which uses one byte per
character, not Unicode.

<P>
<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes achieve great compression ratios - about
15% better than variable byte codes for Reuters-RCV1. But they
are expensive to decode. This is because many
bit-level operations - shifts and masks - are necessary to
decode a sequence of <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes as the boundaries between
codes will usually be somewhere in the middle of a machine
word. As a result, query processing is more expensive for
<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes than for variable byte codes.
Whether we choose variable byte or <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoding 
depends on the characteristics of an application, for example,
on the relative
weights we give to conserving disk space versus maximizing query
response time.

<P>
The compression ratio for the index in
Table <A HREF="#tab:summaryicompress">5.6</A>  is about 25%: 400 MB
(uncompressed, each posting stored as a 32-bit word) versus 101 MB
(<IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $">) and 
116 MB (VB). This shows that both <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> and VB codes meet
the objectives we stated in the beginning of the chapter.
Index compression substantially improves time and space
efficiency of indexes by reducing the amount of disk space needed,
increasing the amount of information that can be kept in
the cache, and speeding up data transfers from disk to memory.
<A NAME="6616"></A> <A NAME="6617"></A>

<P>
<B>Exercises.</B>
<UL>
<LI>Compute variable byte codes for the numbers in
Tables <A HREF="postings-file-compression-1.html#tab:icompresstb2">5.3</A> <A HREF="#tab:icompresstb3">5.5</A> .

<P>
</LI>
<LI>Compute variable byte and <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes for
the
postings list <!-- MATH
 $\langle\kern.5pt$
 -->
<IMG
 WIDTH="11" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img342.png"
 ALT="$\langle\kern.5pt$">777, 17743, 294068, 31251336<IMG
 WIDTH="10" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img120.png"
 ALT="$\rangle$">. Use gaps instead
of docIDs where possible. Write binary codes in 8-bit blocks.

<P>
</LI>
<LI><A NAME="ex:cs276varbyte"></A> <A NAME="p:cs276varbyte"></A> 
Consider the postings list 
<!-- MATH
 $\langle 4, 10, 11, 12, 15, 62, 63, 265, 268, 270, 400\rangle$
 -->
<IMG
 WIDTH="283" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img343.png"
 ALT="$ \langle 4, 10, 11, 12, 15, 62, 63, 265, 268, 270, 400\rangle $">
with a corresponding list of gaps
<!-- MATH
 $\langle 4, 6, 1, 1, 3, 47, 1, 202, 3, 2, 130\rangle$
 -->
<IMG
 WIDTH="211" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img344.png"
 ALT="$ \langle 4, 6, 1, 1, 3, 47, 1, 202, 3, 2, 130\rangle $">.
Assume that the length of the postings list is stored separately, so the system knows when a postings list is complete. 
 Using variable byte encoding:
(i) What is the largest gap you can encode in 1 byte?
(ii) What is the largest gap you can encode in 2 bytes?
(iii) How many bytes will the above postings list require under this encoding? (Count only space for encoding the sequence of numbers.) 

<P>
</LI>
<LI><A NAME="ex:nozerogaps"></A>A little trick is to notice that a gap cannot be of length 0
and that the stuff left to encode after shifting cannot be
0. Based on these observations:
(i) Suggest a modification to variable byte encoding that
allows you to encode slightly larger gaps in the same amount
of space.
(ii) What is the largest gap you can encode in 1 byte?
(iii) What is the largest gap you can encode in 2 bytes? 
(iv) How many bytes will the postings list in Exercise <A HREF="#ex:cs276varbyte">5.3.2</A> 
require under this encoding? 
(Count only space for encoding the sequence of numbers.) 

<P>
</LI>
<LI>From the following sequence of <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $">-coded gaps,
reconstruct first the gap sequence and then the postings
sequence: 1110001110101011111101101111011.

<P>
</LI>
<LI><A NAME="ex:deltacode"></A> <A NAME="p:deltacode"></A>  <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes are relatively
inefficient for large numbers (e.g., 1025 in
Table <A HREF="#tab:icompresstb3">5.5</A> ) as they encode the length of the
offset in inefficient unary code.  <A NAME="6629"></A> <A NAME="6630"></A> <I><IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img282.png"
 ALT="$\delta$"> codes</I> 
differ from <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> codes in that they encode the first
part of the code (<I>length</I>) in <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code instead of
unary code.  The encoding of <I>offset</I> is the
same. For example, the <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img282.png"
 ALT="$\delta$"> code of 7 is 10,0,11 (again,
we add commas for readability). 10,0 is the <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code
for <I>length</I> (2 in this case) and the encoding of <I>offset</I>
(11) is unchanged.  (i) Compute the <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img282.png"
 ALT="$\delta$"> codes for the other
numbers in Table <A HREF="#tab:icompresstb3">5.5</A> .  For what range of numbers
is the <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img282.png"
 ALT="$\delta$"> code shorter than the <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code?
(ii) <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> code beats variable byte code in
Table <A HREF="#tab:summaryicompress">5.6</A>  because the index contains stop words and thus
many small gaps. Show that variable byte code is more
compact if larger gaps dominate. (iii) Compare the
compression ratios of <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img282.png"
 ALT="$\delta$"> code and variable byte code
for a distribution of gaps dominated by large gaps.

<P>
</LI>
<LI>Go through the above calculation of index size and
explicitly state all the approximations that were made to
arrive at Equation&nbsp;<A HREF="#icompresseq1">11</A>.

<P>
</LI>
<LI>For a collection of your choosing, determine the number
of documents and terms and the average length of a
document. (i) How large is the inverted index predicted to be by
Equation&nbsp;<A HREF="#icompresseq1">11</A>? (ii) Implement an indexer that
creates a <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $">-compressed inverted index for the
collection. How large is the actual index?  (iii) Implement an
indexer that uses variable byte encoding. How large is the
variable byte encoded index?

<P>
<BR><P></P>
<DIV ALIGN="CENTER">

<A NAME="6641"></A>
<TABLE CELLPADDING=3>
<CAPTION><STRONG>Table:</STRONG>

Two gap sequences to be merged in blocked sort-based indexing
</CAPTION>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT"><IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoded gap sequence of run 1</TD>
<TD ALIGN="LEFT">1110110111111001011111111110100011111001</TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
<TR><TD ALIGN="LEFT">&nbsp;</TD><TD ALIGN="LEFT"><IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $"> encoded gap sequence of run 2</TD>
<TD ALIGN="LEFT">11111010000111111000100011111110010000011111010101

<P></TD>
<TD ALIGN="LEFT">&nbsp;</TD></TR>
</TABLE>
<A NAME="tab:iconsttb3"></A> <A NAME="p:iconsttb3"></A>  

</DIV>
<BR>

<P>
</LI>
<LI>To be able to hold as many postings as possible in
main memory, it is a good idea to compress intermediate
index files during index construction. (i) This makes
merging runs in blocked sort-based indexing more complicated. As an
example, work out the <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.png"
 ALT="$\gamma $">-encoded merged sequence of
the gaps in Table <A HREF="#tab:iconsttb3">5.7</A> .  (ii) Index construction is
more space efficient when using compression. Would you also
expect it to be faster?

<P>
</LI>
<LI>(i) Show that the size of the vocabulary is finite
according to Zipf's law and infinite according to Heaps'
law.  (ii) Can we derive Heaps' law from Zipf's law?

<P>
</LI>
</UL>

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html1791"
  HREF="references-and-further-reading-5.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/next.png"></A> 
<A NAME="tex2html1785"
  HREF="postings-file-compression-1.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/up.png"></A> 
<A NAME="tex2html1781"
  HREF="variable-byte-codes-1.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/prev.png"></A> 
<A NAME="tex2html1787"
  HREF="contents-1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/contents.png"></A> 
<A NAME="tex2html1789"
  HREF="index-1.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index"
 SRC="http://nlp.stanford.edu/IR-book/html/icons/index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html1792"
  HREF="references-and-further-reading-5.html">References and further reading</A>
<B> Up:</B> <A NAME="tex2html1786"
  HREF="postings-file-compression-1.html">Postings file compression</A>
<B> Previous:</B> <A NAME="tex2html1782"
  HREF="variable-byte-codes-1.html">Variable byte codes</A>
 &nbsp; <B>  <A NAME="tex2html1788"
  HREF="contents-1.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html1790"
  HREF="index-1.html">Index</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
&copy; 2008 Cambridge University Press<br>This is an automatically generated page. In case of formatting errors you may want to look at the <a href=http://informationretrieval.org>PDF edition</a> of the book.<br>
2009-04-07
</ADDRESS>
</BODY>
</HTML>
